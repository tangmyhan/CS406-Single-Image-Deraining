{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f75f88e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T00:30:37.392484Z",
     "iopub.status.busy": "2026-01-01T00:30:37.391885Z",
     "iopub.status.idle": "2026-01-01T00:30:46.439931Z",
     "shell.execute_reply": "2026-01-01T00:30:46.439119Z"
    },
    "papermill": {
     "duration": 9.054189,
     "end_time": "2026-01-01T00:30:46.441426",
     "exception": false,
     "start_time": "2026-01-01T00:30:37.387237",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from PIL import Image\n",
    "from skimage import img_as_ubyte\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from tqdm import tqdm\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "def load_checkpoint(model, weights):\n",
    "    checkpoint = torch.load(weights, map_location='cpu')\n",
    "    try:\n",
    "        model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    except:\n",
    "        state_dict = checkpoint[\"state_dict\"]\n",
    "        new_state_dict = OrderedDict()\n",
    "        for k, v in state_dict.items():\n",
    "            name = k[7:] if k.startswith('module.') else k\n",
    "            new_state_dict[name] = v\n",
    "        model.load_state_dict(new_state_dict)\n",
    "\n",
    "def load_start_epoch(weights):\n",
    "    checkpoint = torch.load(weights)\n",
    "    epoch = checkpoint[\"epoch\"]\n",
    "    return epoch\n",
    "\n",
    "def mkdir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1a16541",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T00:30:46.448908Z",
     "iopub.status.busy": "2026-01-01T00:30:46.448028Z",
     "iopub.status.idle": "2026-01-01T00:30:46.454233Z",
     "shell.execute_reply": "2026-01-01T00:30:46.453661Z"
    },
    "papermill": {
     "duration": 0.010679,
     "end_time": "2026-01-01T00:30:46.455234",
     "exception": false,
     "start_time": "2026-01-01T00:30:46.444555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def torchPSNR(tar_img, prd_img):\n",
    "    imdff = torch.clamp(prd_img, 0, 1) - torch.clamp(tar_img, 0, 1)\n",
    "    rmse = (imdff**2).mean().sqrt()\n",
    "    ps = 20 * torch.log10(1 / rmse)\n",
    "    return ps\n",
    "\n",
    "def save_img(filepath, img):\n",
    "    cv2.imwrite(filepath, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "def numpyPSNR(tar_img, prd_img):\n",
    "    imdff = np.float32(prd_img) - np.float32(tar_img)\n",
    "    rmse = np.sqrt(np.mean(imdff**2))\n",
    "    ps = 20 * np.log10(255 / rmse)\n",
    "    return ps\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in ['jpeg', 'JPEG', 'jpg', 'png', 'JPG', 'PNG', 'gif'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52e42820",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T00:30:46.461756Z",
     "iopub.status.busy": "2026-01-01T00:30:46.461366Z",
     "iopub.status.idle": "2026-01-01T00:30:46.466395Z",
     "shell.execute_reply": "2026-01-01T00:30:46.465770Z"
    },
    "papermill": {
     "duration": 0.009556,
     "end_time": "2026-01-01T00:30:46.467526",
     "exception": false,
     "start_time": "2026-01-01T00:30:46.457970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataLoaderTest(torch.utils.data.Dataset):\n",
    "    def __init__(self, inp_dir, img_options):\n",
    "        super(DataLoaderTest, self).__init__()\n",
    "        inp_files = sorted(os.listdir(inp_dir))\n",
    "        self.inp_filenames = [os.path.join(inp_dir, x) for x in inp_files if is_image_file(x)]\n",
    "        self.inp_size = len(self.inp_filenames)\n",
    "        self.img_options = img_options\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.inp_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path_inp = self.inp_filenames[index]\n",
    "        filename = os.path.splitext(os.path.split(path_inp)[-1])[0]\n",
    "        inp = Image.open(path_inp)\n",
    "        inp = TF.to_tensor(inp)\n",
    "        return inp, filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76e99975",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T00:30:46.474456Z",
     "iopub.status.busy": "2026-01-01T00:30:46.474012Z",
     "iopub.status.idle": "2026-01-01T00:30:46.487013Z",
     "shell.execute_reply": "2026-01-01T00:30:46.486515Z"
    },
    "papermill": {
     "duration": 0.01761,
     "end_time": "2026-01-01T00:30:46.487943",
     "exception": false,
     "start_time": "2026-01-01T00:30:46.470333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def window_partitions(x, window_size):\n",
    "    if isinstance(window_size, int):\n",
    "        window_size = [window_size, window_size]\n",
    "    B, C, H, W = x.shape\n",
    "    x = x.view(B, C, H // window_size[0], window_size[0], W // window_size[1], window_size[1])\n",
    "    windows = x.permute(0, 2, 4, 1, 3, 5).contiguous().view(-1, C, window_size[0], window_size[1])\n",
    "    return windows\n",
    "\n",
    "def window_reverses(windows, window_size, H, W):\n",
    "    if isinstance(window_size, int):\n",
    "        window_size = [window_size, window_size]\n",
    "    C = windows.shape[1]\n",
    "    x = windows.view(-1, H // window_size[0], W // window_size[1], C, window_size[0], window_size[1])\n",
    "    x = x.permute(0, 3, 1, 4, 2, 5).contiguous().view(-1, C, H, W)\n",
    "    return x\n",
    "\n",
    "def window_partitionx(x, window_size):\n",
    "    _, _, H, W = x.shape\n",
    "    h, w = window_size * (H // window_size), window_size * (W // window_size)\n",
    "    x_main = window_partitions(x[:, :, :h, :w], window_size)\n",
    "    b_main = x_main.shape[0]\n",
    "    if h == H and w == W:\n",
    "        return x_main, [b_main]\n",
    "    if h != H and w != W:\n",
    "        x_r = window_partitions(x[:, :, :h, -window_size:], window_size)\n",
    "        b_r = x_r.shape[0] + b_main\n",
    "        x_d = window_partitions(x[:, :, -window_size:, :w], window_size)\n",
    "        b_d = x_d.shape[0] + b_r\n",
    "        x_dd = x[:, :, -window_size:, -window_size:]\n",
    "        b_dd = x_dd.shape[0] + b_d\n",
    "        return torch.cat([x_main, x_r, x_d, x_dd], dim=0), [b_main, b_r, b_d, b_dd]\n",
    "    if h == H and w != W:\n",
    "        x_r = window_partitions(x[:, :, :h, -window_size:], window_size)\n",
    "        b_r = x_r.shape[0] + b_main\n",
    "        return torch.cat([x_main, x_r], dim=0), [b_main, b_r]\n",
    "    if h != H and w == W:\n",
    "        x_d = window_partitions(x[:, :, -window_size:, :w], window_size)\n",
    "        b_d = x_d.shape[0] + b_main\n",
    "        return torch.cat([x_main, x_d], dim=0), [b_main, b_d]\n",
    "\n",
    "def window_reversex(windows, window_size, H, W, batch_list):\n",
    "    h, w = window_size * (H // window_size), window_size * (W // window_size)\n",
    "    x_main = window_reverses(windows[:batch_list[0], ...], window_size, h, w)\n",
    "    B, C, _, _ = x_main.shape\n",
    "    if torch.is_complex(windows):\n",
    "        res = torch.complex(torch.zeros([B, C, H, W]), torch.zeros([B, C, H, W]))\n",
    "        res = res.to(windows.device)\n",
    "    else:\n",
    "        res = torch.zeros([B, C, H, W], device=windows.device)\n",
    "    res[:, :, :h, :w] = x_main\n",
    "    if h == H and w == W:\n",
    "        return res\n",
    "    if h != H and w != W and len(batch_list) == 4:\n",
    "        x_dd = window_reverses(windows[batch_list[2]:, ...], window_size, window_size, window_size)\n",
    "        res[:, :, h:, w:] = x_dd[:, :, h - H:, w - W:]\n",
    "        x_r = window_reverses(windows[batch_list[0]:batch_list[1], ...], window_size, h, window_size)\n",
    "        res[:, :, :h, w:] = x_r[:, :, :, w - W:]\n",
    "        x_d = window_reverses(windows[batch_list[1]:batch_list[2], ...], window_size, window_size, w)\n",
    "        res[:, :, h:, :w] = x_d[:, :, h - H:, :]\n",
    "        return res\n",
    "    if w != W and len(batch_list) == 2:\n",
    "        x_r = window_reverses(windows[batch_list[0]:batch_list[1], ...], window_size, h, window_size)\n",
    "        res[:, :, :h, w:] = x_r[:, :, :, w - W:]\n",
    "    if h != H and len(batch_list) == 2:\n",
    "        x_d = window_reverses(windows[batch_list[0]:batch_list[1], ...], window_size, window_size, w)\n",
    "        res[:, :, h:, :w] = x_d[:, :, h - H:, :]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d4d0186",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T00:30:46.494452Z",
     "iopub.status.busy": "2026-01-01T00:30:46.494205Z",
     "iopub.status.idle": "2026-01-01T00:30:46.511941Z",
     "shell.execute_reply": "2026-01-01T00:30:46.511385Z"
    },
    "papermill": {
     "duration": 0.022427,
     "end_time": "2026-01-01T00:30:46.513038",
     "exception": false,
     "start_time": "2026-01-01T00:30:46.490611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "\n",
    "hidden_list = [256, 256, 256]\n",
    "L = 4\n",
    "\n",
    "def make_coord(shape, ranges=None, flatten=True):\n",
    "    coord_seqs = []\n",
    "    for i, n in enumerate(shape):\n",
    "        if ranges is None:\n",
    "            v0, v1 = -1, 1\n",
    "        else:\n",
    "            v0, v1 = ranges[i]\n",
    "        r = (v1 - v0) / (2 * n)\n",
    "        seq = v0 + r + (2 * r) * torch.arange(n).float()\n",
    "        coord_seqs.append(seq)\n",
    "    ret = torch.stack(torch.meshgrid(*coord_seqs), dim=-1)\n",
    "    if flatten:\n",
    "        ret = ret.view(-1, ret.shape[-1])\n",
    "    return ret\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hidden_list):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        lastv = in_dim\n",
    "        for hidden in hidden_list:\n",
    "            layers.append(nn.Linear(lastv, hidden))\n",
    "            layers.append(nn.ReLU())\n",
    "            lastv = hidden\n",
    "        layers.append(nn.Linear(lastv, out_dim))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shape = x.shape[:-1]\n",
    "        x = self.layers(x.view(-1, x.shape[-1]))\n",
    "        return x.view(*shape, -1)\n",
    "\n",
    "class INR(nn.Module):\n",
    "    def __init__(self, dim, local_ensemble=True, feat_unfold=True, cell_decode=True):\n",
    "        super().__init__()\n",
    "        self.local_ensemble = local_ensemble\n",
    "        self.feat_unfold = feat_unfold\n",
    "        self.cell_decode = cell_decode\n",
    "        imnet_in_dim = dim\n",
    "\n",
    "        if self.feat_unfold:\n",
    "            imnet_in_dim *= 9\n",
    "        imnet_in_dim += 2 + 4 * L  \n",
    "        if self.cell_decode:\n",
    "            imnet_in_dim += 2\n",
    "\n",
    "        self.imnet = MLP(imnet_in_dim, 3, hidden_list)\n",
    "\n",
    "    def query_rgb(self, inp, coord, cell=None):\n",
    "        feat = inp\n",
    "        if self.feat_unfold:\n",
    "            feat = F.unfold(feat, 3, padding=1).view(\n",
    "                feat.shape[0], feat.shape[1] * 9, feat.shape[2], feat.shape[3])\n",
    "\n",
    "        if self.local_ensemble:\n",
    "            vx_lst = [-1, 1]\n",
    "            vy_lst = [-1, 1]\n",
    "            eps_shift = 1e-6\n",
    "        else:\n",
    "            vx_lst, vy_lst, eps_shift = [0], [0], 0\n",
    "\n",
    "        rx = 2 / feat.shape[-2] / 2\n",
    "        ry = 2 / feat.shape[-1] / 2\n",
    "        \n",
    "        device = inp.device\n",
    "        feat_coord = make_coord(feat.shape[-2:], flatten=False).to(device) \\\n",
    "            .permute(2, 0, 1) \\\n",
    "            .unsqueeze(0).expand(feat.shape[0], 2, *feat.shape[-2:])\n",
    "\n",
    "        preds = []\n",
    "        areas = []\n",
    "        for vx in vx_lst:\n",
    "            for vy in vy_lst:\n",
    "                coord_ = coord.clone()\n",
    "                coord_[:, :, 0] += vx * rx + eps_shift\n",
    "                coord_[:, :, 1] += vy * ry + eps_shift\n",
    "                coord_.clamp_(-1 + 1e-6, 1 - 1e-6)\n",
    "\n",
    "                bs, q, h, w = feat.shape\n",
    "                q_feat = feat.view(bs, q, -1).permute(0, 2, 1)\n",
    "\n",
    "                bs, q, h, w = feat_coord.shape\n",
    "                q_coord = feat_coord.view(bs, q, -1).permute(0, 2, 1)\n",
    "\n",
    "                points_enc = self.positional_encoding(q_coord, L=L)\n",
    "                q_coord = torch.cat([q_coord, points_enc], dim=-1)  \n",
    "\n",
    "                rel_coord = coord - q_coord\n",
    "                rel_coord[:, :, 0] *= feat.shape[-2]\n",
    "                rel_coord[:, :, 1] *= feat.shape[-1]\n",
    "                inp = torch.cat([q_feat, rel_coord], dim=-1)\n",
    "\n",
    "                if self.cell_decode:\n",
    "                    rel_cell = cell.clone()\n",
    "                    rel_cell[:, :, 0] *= feat.shape[-2]\n",
    "                    rel_cell[:, :, 1] *= feat.shape[-1]\n",
    "                    inp = torch.cat([inp, rel_cell], dim=-1)\n",
    "\n",
    "                bs, q = coord.shape[:2]\n",
    "                pred = self.imnet(inp.view(bs * q, -1)).view(bs, q, -1)\n",
    "                preds.append(pred)\n",
    "\n",
    "                area = torch.abs(rel_coord[:, :, 0] * rel_coord[:, :, 1])\n",
    "                areas.append(area + 1e-9)\n",
    "\n",
    "        tot_area = torch.stack(areas).sum(dim=0)\n",
    "        if self.local_ensemble:\n",
    "            t = areas[0];\n",
    "            areas[0] = areas[3];\n",
    "            areas[3] = t\n",
    "            t = areas[1];\n",
    "            areas[1] = areas[2];\n",
    "            areas[2] = t\n",
    "        ret = 0\n",
    "        for pred, area in zip(preds, areas):\n",
    "            ret = ret + pred * (area / tot_area).unsqueeze(-1)\n",
    "\n",
    "        bs, q, h, w = feat.shape\n",
    "        ret = ret.view(bs, h, w, -1).permute(0, 3, 1, 2)\n",
    "        return ret\n",
    "\n",
    "    def forward(self, inp):\n",
    "        h, w = inp.shape[2], inp.shape[3]\n",
    "        B = inp.shape[0]\n",
    "        #coord = make_coord((h, w)).cuda() # cũ\n",
    "        device = inp.device\n",
    "        coord = make_coord((h, w)).to(device)\n",
    "        cell = torch.ones_like(coord)\n",
    "        cell[:, 0] *= 2 / h\n",
    "        cell[:, 1] *= 2 / w\n",
    "        cell = cell.unsqueeze(0).repeat(B, 1, 1)\n",
    "        coord = coord.unsqueeze(0).repeat(B, 1, 1)\n",
    "        points_enc = self.positional_encoding(coord, L=L)\n",
    "        coord = torch.cat([coord, points_enc], dim=-1)  \n",
    "\n",
    "        return self.query_rgb(inp, coord, cell)\n",
    "\n",
    "    def positional_encoding(self, input, L): \n",
    "        shape = input.shape\n",
    "        device = input.device\n",
    "        freq = 2 ** torch.arange(L, dtype=torch.float32).to(device) * np.pi  \n",
    "        spectrum = input[..., None] * freq  \n",
    "        sin, cos = spectrum.sin(), spectrum.cos()  \n",
    "        input_enc = torch.stack([sin, cos], dim=-2)  \n",
    "        input_enc = input_enc.view(*shape[:-1], -1)  \n",
    "\n",
    "        return input_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15c46890",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T00:30:46.519357Z",
     "iopub.status.busy": "2026-01-01T00:30:46.519129Z",
     "iopub.status.idle": "2026-01-01T00:30:46.557817Z",
     "shell.execute_reply": "2026-01-01T00:30:46.557318Z"
    },
    "papermill": {
     "duration": 0.04319,
     "end_time": "2026-01-01T00:30:46.558875",
     "exception": false,
     "start_time": "2026-01-01T00:30:46.515685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import numbers\n",
    "from einops import rearrange\n",
    "\n",
    "def to_3d(x):\n",
    "    return rearrange(x, 'b c h w -> b (h w) c')\n",
    "\n",
    "\n",
    "def to_4d(x, h, w):\n",
    "    return rearrange(x, 'b (h w) c -> b c h w', h=h, w=w)\n",
    "\n",
    "\n",
    "class BasicConv(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, kernel_size, stride, bias=False, norm=False, relu=True, transpose=False,\n",
    "                 channel_shuffle_g=0, norm_method=nn.BatchNorm2d, groups=1):\n",
    "        super(BasicConv, self).__init__()\n",
    "        self.channel_shuffle_g = channel_shuffle_g\n",
    "        self.norm = norm\n",
    "        if bias and norm:\n",
    "            bias = False\n",
    "\n",
    "        padding = kernel_size // 2\n",
    "        layers = list()\n",
    "        if transpose:\n",
    "            padding = kernel_size // 2 - 1\n",
    "            layers.append(\n",
    "                nn.ConvTranspose2d(in_channel, out_channel, kernel_size, padding=padding, stride=stride, bias=bias,\n",
    "                                   groups=groups))\n",
    "        else:\n",
    "            layers.append(\n",
    "                nn.Conv2d(in_channel, out_channel, kernel_size, padding=padding, stride=stride, bias=bias,\n",
    "                          groups=groups))\n",
    "        if norm:\n",
    "            layers.append(norm_method(out_channel))\n",
    "        elif relu:\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "\n",
    "        self.main = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "\n",
    "\n",
    "class BiasFree_LayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape):\n",
    "        super(BiasFree_LayerNorm, self).__init__()\n",
    "        if isinstance(normalized_shape, numbers.Integral):\n",
    "            normalized_shape = (normalized_shape,)\n",
    "        normalized_shape = torch.Size(normalized_shape)\n",
    "\n",
    "        assert len(normalized_shape) == 1\n",
    "\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.normalized_shape = normalized_shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        sigma = x.var(-1, keepdim=True, unbiased=False)\n",
    "        return x / torch.sqrt(sigma + 1e-5) * self.weight\n",
    "\n",
    "\n",
    "class WithBias_LayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape):\n",
    "        super(WithBias_LayerNorm, self).__init__()\n",
    "        if isinstance(normalized_shape, numbers.Integral):\n",
    "            normalized_shape = (normalized_shape,)\n",
    "        normalized_shape = torch.Size(normalized_shape)\n",
    "\n",
    "        assert len(normalized_shape) == 1\n",
    "\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
    "        self.normalized_shape = normalized_shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu = x.mean(-1, keepdim=True)\n",
    "        sigma = x.var(-1, keepdim=True, unbiased=False)\n",
    "        return (x - mu) / torch.sqrt(sigma + 1e-5) * self.weight + self.bias\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim, LayerNorm_type):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        if LayerNorm_type == 'BiasFree':\n",
    "            self.body = BiasFree_LayerNorm(dim)\n",
    "        else:\n",
    "            self.body = WithBias_LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, w = x.shape[-2:]\n",
    "        return to_4d(self.body(to_3d(x)), h, w)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, ffn_expansion_factor, bias, BasicConv=BasicConv):\n",
    "        super(FeedForward, self).__init__()\n",
    "\n",
    "        hidden_features = int(dim * ffn_expansion_factor)\n",
    "\n",
    "        self.project_in = nn.Conv2d(dim, hidden_features * 2, kernel_size=1, bias=bias)\n",
    "\n",
    "        self.dwconv = BasicConv(hidden_features * 2, hidden_features * 2, kernel_size=3, stride=1, bias=bias,\n",
    "                                relu=False, groups=hidden_features * 2)\n",
    "\n",
    "        self.project_out = nn.Conv2d(hidden_features, dim, kernel_size=1, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.project_in(x)\n",
    "        x1, x2 = self.dwconv(x).chunk(2, dim=1)\n",
    "        x = F.gelu(x1) * x2\n",
    "        x = self.project_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads, bias, BasicConv=BasicConv):\n",
    "        super(Attention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))\n",
    "\n",
    "        self.qkv = nn.Conv2d(dim, dim * 3, kernel_size=1, bias=bias)\n",
    "        self.qkv_dwconv = BasicConv(dim * 3, dim * 3, kernel_size=3, stride=1, bias=bias, relu=False, groups=dim * 3)\n",
    "        self.project_out = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "\n",
    "        qkv = self.qkv_dwconv(self.qkv(x))\n",
    "        q, k, v = qkv.chunk(3, dim=1)\n",
    "\n",
    "        q = rearrange(q, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
    "        k = rearrange(k, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
    "        v = rearrange(v, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
    "\n",
    "        q = torch.nn.functional.normalize(q, dim=-1)\n",
    "        k = torch.nn.functional.normalize(k, dim=-1)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.temperature\n",
    "        attn = attn.softmax(dim=-1)\n",
    "\n",
    "        out = (attn @ v)\n",
    "\n",
    "        out = rearrange(out, 'b head c (h w) -> b (head c) h w', head=self.num_heads, h=h, w=w)\n",
    "\n",
    "        out = self.project_out(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, ffn_expansion_factor, bias, LayerNorm_type, BasicConv=BasicConv):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        self.norm1 = LayerNorm(dim, LayerNorm_type)\n",
    "        self.attn = Attention(dim, num_heads, bias, BasicConv=BasicConv)\n",
    "        self.norm2 = LayerNorm(dim, LayerNorm_type)\n",
    "        self.ffn = FeedForward(dim, ffn_expansion_factor, bias, BasicConv=BasicConv)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class OverlapPatchEmbed(nn.Module):\n",
    "    def __init__(self, in_c=3, embed_dim=48, bias=False):\n",
    "        super(OverlapPatchEmbed, self).__init__()\n",
    "\n",
    "        self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=3, stride=1, padding=1, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Downsample(nn.Module):\n",
    "    def __init__(self, n_feat):\n",
    "        super(Downsample, self).__init__()\n",
    "\n",
    "        self.body = nn.Sequential(nn.Conv2d(n_feat, n_feat // 2, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                                  nn.PixelUnshuffle(2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.body(x)\n",
    "\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    def __init__(self, n_feat):\n",
    "        super(Upsample, self).__init__()\n",
    "\n",
    "        self.body = nn.Sequential(nn.Conv2d(n_feat, n_feat * 2, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                                  nn.PixelShuffle(2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.body(x)\n",
    "\n",
    "\n",
    "class Fusion(nn.Module):\n",
    "    def __init__(self, in_dim=32):\n",
    "        super(Fusion, self).__init__()\n",
    "        self.chanel_in = in_dim\n",
    "\n",
    "        self.query_conv = nn.Conv2d(in_dim, in_dim, 3, 1, 1, bias=True)\n",
    "        self.key_conv = nn.Conv2d(in_dim, in_dim, 3, 1, 1, bias=True)\n",
    "\n",
    "        self.gamma1 = nn.Conv2d(in_dim * 2, 2, 3, 1, 1, bias=True)\n",
    "        self.gamma2 = nn.Conv2d(in_dim * 2, 2, 3, 1, 1, bias=True)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x_q = self.query_conv(x)\n",
    "        y_k = self.key_conv(y)\n",
    "        energy = x_q * y_k\n",
    "        attention = self.sig(energy)\n",
    "        attention_x = x * attention\n",
    "        attention_y = y * attention\n",
    "\n",
    "        x_gamma = self.gamma1(torch.cat((x, attention_x), dim=1))\n",
    "        x_out = x * x_gamma[:, [0], :, :] + attention_x * x_gamma[:, [1], :, :]\n",
    "\n",
    "        y_gamma = self.gamma2(torch.cat((y, attention_y), dim=1))\n",
    "        y_out = y * y_gamma[:, [0], :, :] + attention_y * y_gamma[:, [1], :, :]\n",
    "\n",
    "        x_s = x_out + y_out\n",
    "\n",
    "        return x_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "604f833d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T00:30:46.565678Z",
     "iopub.status.busy": "2026-01-01T00:30:46.565450Z",
     "iopub.status.idle": "2026-01-01T00:30:46.739030Z",
     "shell.execute_reply": "2026-01-01T00:30:46.738470Z"
    },
    "papermill": {
     "duration": 0.178788,
     "end_time": "2026-01-01T00:30:46.740430",
     "exception": false,
     "start_time": "2026-01-01T00:30:46.561642",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiscaleNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 inp_channels=3,\n",
    "                 out_channels=3,\n",
    "                 dim=48,\n",
    "                 num_blocks=[2, 3, 3],\n",
    "                 heads=[1, 2, 4],\n",
    "                 ffn_expansion_factor=2.66,\n",
    "                 bias=False,\n",
    "                 LayerNorm_type='WithBias',\n",
    "                 ):\n",
    "        super(MultiscaleNet, self).__init__()\n",
    "        self.patch_embed_small = OverlapPatchEmbed(inp_channels, dim)\n",
    "\n",
    "        self.encoder_level1_small = nn.Sequential(*[\n",
    "            TransformerBlock(dim=dim, num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias,\n",
    "                             LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n",
    "\n",
    "        self.down1_2_small = Downsample(dim)\n",
    "        self.encoder_level2_small = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 2 ** 1), num_heads=heads[1], ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[1])])\n",
    "\n",
    "        self.down2_3_small = Downsample(int(dim * 2 ** 1))\n",
    "        self.latent_small = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 2 ** 2), num_heads=heads[2], ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[2])])\n",
    "\n",
    "        self.up3_2_small = Upsample(int(dim * 2 ** 2))\n",
    "        self.reduce_chan_level2_small = nn.Conv2d(int(dim * 2 ** 2), int(dim * 2 ** 1), kernel_size=1, bias=bias)\n",
    "        self.decoder_level2_small = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 2 ** 1), num_heads=heads[1], ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[1])])\n",
    "\n",
    "        self.up2_1_small = Upsample(int(dim * 2 ** 1))\n",
    "        self.reduce_chan_level1_small = nn.Conv2d(int(dim * 2 ** 1), int(dim * 1 ** 1), kernel_size=1, bias=bias)\n",
    "        self.decoder_level1_small = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 1 ** 1), num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n",
    "\n",
    "        self.output_small = nn.Conv2d(int(dim * 1 ** 1), out_channels, kernel_size=3, stride=1, padding=1, bias=bias)\n",
    "\n",
    "        #self.INR = INR(dim).cuda() # cũ\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.INR = INR(dim).to(device) # mới\n",
    "\n",
    "        self.patch_embed_mid = OverlapPatchEmbed(inp_channels, dim)\n",
    "\n",
    "        self.encoder_level1_mid1 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=dim, num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias,\n",
    "                             LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n",
    "\n",
    "        self.encoder_level1_mid2 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=dim, num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias,\n",
    "                             LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n",
    "\n",
    "        self.down1_2_mid = Downsample(dim)\n",
    "        self.down1_2_mid2 = Downsample(dim)\n",
    "        self.encoder_level2_mid1 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 2 ** 1), num_heads=heads[1], ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[1])])\n",
    "        self.encoder_level2_mid2 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 2 ** 1), num_heads=heads[1], ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[1])])\n",
    "\n",
    "        self.down2_3_mid = Downsample(int(dim * 2 ** 1))\n",
    "        self.down2_3_mid2 = Downsample(int(dim * 2 ** 1))\n",
    "        self.latent_mid1 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 2 ** 2), num_heads=heads[2], ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[2])])\n",
    "        self.latent_mid2 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 2 ** 2), num_heads=heads[2], ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[2])])\n",
    "\n",
    "        self.up3_2_mid = Upsample(int(dim * 2 ** 2))\n",
    "        self.up3_2_mid2 = Upsample(int(dim * 2 ** 2))\n",
    "        self.reduce_chan_level2_mid1 = nn.Conv2d(int(dim * 2 ** 2), int(dim * 2 ** 1), kernel_size=1, bias=bias)\n",
    "        self.reduce_chan_level2_mid2 = nn.Conv2d(int(dim * 2 ** 2), int(dim * 2 ** 1), kernel_size=1, bias=bias)\n",
    "        self.decoder_level2_mid1 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 2 ** 1), num_heads=heads[1], ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[1])])\n",
    "        self.decoder_level2_mid2 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 2 ** 1), num_heads=heads[1], ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[1])])\n",
    "\n",
    "        self.up2_1_mid = Upsample(int(dim * 2 ** 1))\n",
    "        self.up2_1_mid2 = Upsample(int(dim * 2 ** 1))\n",
    "        self.reduce_chan_level1_mid1 = nn.Conv2d(int(dim * 2 ** 1), int(dim * 1 ** 1), kernel_size=1, bias=bias)\n",
    "        self.reduce_chan_level1_mid2 = nn.Conv2d(int(dim * 2 ** 1), int(dim * 1 ** 1), kernel_size=1, bias=bias)\n",
    "        self.decoder_level1_mid1 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 1 ** 1), num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n",
    "        self.decoder_level1_mid2 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 1 ** 1), num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n",
    "\n",
    "        self.output_mid = nn.Conv2d(int(dim * 1 ** 1), out_channels, kernel_size=3, stride=1, padding=1, bias=bias)\n",
    "        self.output_mid_context = nn.Conv2d(int(dim * 1 ** 1), dim, kernel_size=3, stride=1, padding=1, bias=bias)\n",
    "\n",
    "        #self.INR2 = INR(dim).cuda() # cũ\n",
    "\n",
    "        self.INR2 = INR(dim).to(device) # mới\n",
    "\n",
    "        self.patch_embed_max = OverlapPatchEmbed(inp_channels, dim)\n",
    "\n",
    "        self.encoder_level1_max1 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=dim, num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias,\n",
    "                             LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n",
    "        self.encoder_level1_max2 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=dim, num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias,\n",
    "                             LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n",
    "        self.encoder_level1_max3 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=dim, num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias,\n",
    "                             LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n",
    "\n",
    "        self.down1_2_max = Downsample(dim)\n",
    "        self.down1_2_max2 = Downsample(dim)\n",
    "        self.down1_2_max3 = Downsample(dim)\n",
    "        self.encoder_level2_max1 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 2 ** 1), num_heads=heads[1], ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[1])])\n",
    "        self.encoder_level2_max2 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 2 ** 1), num_heads=heads[1], ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[1])])\n",
    "        self.encoder_level2_max3 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 2 ** 1), num_heads=heads[1], ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[1])])\n",
    "\n",
    "        self.down2_3_max = Downsample(int(dim * 2 ** 1))\n",
    "        self.down2_3_max2 = Downsample(int(dim * 2 ** 1))\n",
    "        self.down2_3_max3 = Downsample(int(dim * 2 ** 1))\n",
    "        self.latent_max1 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 2 ** 2), num_heads=heads[2], ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[2])])\n",
    "        self.latent_max2 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 2 ** 2), num_heads=heads[2], ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[2])])\n",
    "        self.latent_max3 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 2 ** 2), num_heads=heads[2], ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[2])])\n",
    "\n",
    "        self.up3_2_max = Upsample(int(dim * 2 ** 2))\n",
    "        self.up3_2_max2 = Upsample(int(dim * 2 ** 2))\n",
    "        self.up3_2_max3 = Upsample(int(dim * 2 ** 2))\n",
    "        self.reduce_chan_level2_max1 = nn.Conv2d(int(dim * 2 ** 2), int(dim * 2 ** 1), kernel_size=1, bias=bias)\n",
    "        self.reduce_chan_level2_max2 = nn.Conv2d(int(dim * 2 ** 2), int(dim * 2 ** 1), kernel_size=1, bias=bias)\n",
    "        self.reduce_chan_level2_max3 = nn.Conv2d(int(dim * 2 ** 2), int(dim * 2 ** 1), kernel_size=1, bias=bias)\n",
    "        self.decoder_level2_max1 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 2 ** 1), num_heads=heads[1], ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[1])])\n",
    "        self.decoder_level2_max2 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 2 ** 1), num_heads=heads[1], ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[1])])\n",
    "        self.decoder_level2_max3 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 2 ** 1), num_heads=heads[1], ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[1])])\n",
    "\n",
    "        self.up2_1_max = Upsample(int(dim * 2 ** 1))\n",
    "        self.up2_1_max2 = Upsample(int(dim * 2 ** 1))\n",
    "        self.up2_1_max3 = Upsample(int(dim * 2 ** 1))\n",
    "        self.reduce_chan_level1_max1 = nn.Conv2d(int(dim * 2 ** 1), int(dim * 1 ** 1), kernel_size=1, bias=bias)\n",
    "        self.reduce_chan_level1_max2 = nn.Conv2d(int(dim * 2 ** 1), int(dim * 1 ** 1), kernel_size=1, bias=bias)\n",
    "        self.reduce_chan_level1_max3 = nn.Conv2d(int(dim * 2 ** 1), int(dim * 1 ** 1), kernel_size=1, bias=bias)\n",
    "        self.decoder_level1_max1 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 1 ** 1), num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n",
    "        self.decoder_level1_max2 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 1 ** 1), num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n",
    "        self.decoder_level1_max3 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 1 ** 1), num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n",
    "\n",
    "        self.output_max = nn.Conv2d(int(dim * 1 ** 1), out_channels, kernel_size=3, stride=1, padding=1, bias=bias)\n",
    "        self.output_max_context1 = nn.Conv2d(int(dim * 1 ** 1), dim, kernel_size=3, stride=1, padding=1, bias=bias)\n",
    "        self.output_max_context2 = nn.Conv2d(int(dim * 1 ** 1), dim, kernel_size=3, stride=1, padding=1, bias=bias)\n",
    "\n",
    "        self.BF1 = Fusion(dim * 4)\n",
    "        self.BF2 = Fusion(dim * 4)\n",
    "        self.BF3 = Fusion(dim * 4)\n",
    "\n",
    "        self.upsmall2mid1 = Upsample(int(dim * 4 ** 1))\n",
    "        self.upsmall2mid2 = Upsample(int(dim * 2 ** 1))\n",
    "\n",
    "        self.upmid2max1 = Upsample(int(dim * 4 ** 1))\n",
    "        self.upmid2max2 = Upsample(int(dim * 2 ** 1))\n",
    "\n",
    "    def forward(self, inp_img):\n",
    "        outputs = list()\n",
    "\n",
    "        inp_img_max = inp_img\n",
    "        inp_img_mid = F.interpolate(inp_img, scale_factor=0.5)\n",
    "        inp_img_small = F.interpolate(inp_img, scale_factor=0.25)\n",
    "\n",
    "        inp_enc_level1_small = self.patch_embed_small(inp_img_small)\n",
    "        out_enc_level1_small = self.encoder_level1_small(inp_enc_level1_small)\n",
    "\n",
    "        inp_enc_level2_small = self.down1_2_small(out_enc_level1_small)\n",
    "        out_enc_level2_small = self.encoder_level2_small(inp_enc_level2_small)\n",
    "\n",
    "        inp_enc_level4_small = self.down2_3_small(out_enc_level2_small)\n",
    "        latent_small = self.latent_small(inp_enc_level4_small)\n",
    "        latent_small_mid = self.upsmall2mid1(latent_small)\n",
    "        latent_small_mid = self.upsmall2mid2(latent_small_mid)\n",
    "\n",
    "        outputs.append(inp_img_small)\n",
    "        INR = self.INR(latent_small_mid)\n",
    "        inp_img_small_ = INR + inp_img_small\n",
    "        outputs.append(inp_img_small_)\n",
    "\n",
    "        inp_img_small_ = F.interpolate(inp_img_small_, scale_factor=2)\n",
    "\n",
    "        mid_img = inp_img_mid + inp_img_small_\n",
    "\n",
    "        inp_enc_level1_mid = self.patch_embed_mid(mid_img)\n",
    "        out_enc_level1_mid = self.encoder_level1_mid1(inp_enc_level1_mid)\n",
    "\n",
    "        inp_enc_level2_mid = self.down1_2_mid(out_enc_level1_mid)\n",
    "        out_enc_level2_mid = self.encoder_level2_mid1(inp_enc_level2_mid)\n",
    "\n",
    "        inp_enc_level4_mid = self.down2_3_mid(out_enc_level2_mid)\n",
    "        latent_mid = self.latent_mid1(inp_enc_level4_mid)\n",
    "        latent_mid_INR_max = self.upmid2max1(latent_mid)\n",
    "        latent_mid_INR_max = self.upmid2max2(latent_mid_INR_max)\n",
    "\n",
    "        outputs.append(mid_img / 2)\n",
    "        INR2 = self.INR2(latent_mid_INR_max)\n",
    "        mid_img_ = INR2 + mid_img\n",
    "        outputs.append(mid_img_)\n",
    "\n",
    "        mid_img_ = F.interpolate(mid_img_, scale_factor=2)\n",
    "\n",
    "        max_img = inp_img_max + mid_img_\n",
    "\n",
    "        inp_enc_level1_max = self.patch_embed_max(max_img)\n",
    "        out_enc_level1_max = self.encoder_level1_max1(inp_enc_level1_max)\n",
    "\n",
    "        inp_enc_level2_max = self.down1_2_max(out_enc_level1_max)\n",
    "        out_enc_level2_max = self.encoder_level2_max1(inp_enc_level2_max)\n",
    "\n",
    "        inp_enc_level4_max = self.down2_3_max(out_enc_level2_max)\n",
    "        latent_max = self.latent_max1(inp_enc_level4_max)\n",
    "        BFF_max_1 = latent_max\n",
    "\n",
    "        inp_dec_level2_max = self.up3_2_max(latent_max)\n",
    "        inp_dec_level2_max = torch.cat([inp_dec_level2_max, out_enc_level2_max], 1)\n",
    "        inp_dec_level2_max = self.reduce_chan_level2_max1(inp_dec_level2_max)\n",
    "        out_dec_level2_max = self.decoder_level2_max1(inp_dec_level2_max)\n",
    "\n",
    "        inp_dec_level1_max = self.up2_1_max(out_dec_level2_max)\n",
    "        inp_dec_level1_max = torch.cat([inp_dec_level1_max, out_enc_level1_max], 1)\n",
    "        inp_dec_level1_max = self.reduce_chan_level1_max1(inp_dec_level1_max)\n",
    "        out_dec_level1_max = self.decoder_level1_max1(inp_dec_level1_max)\n",
    "\n",
    "        out_dec_level1_max = self.output_max_context1(out_dec_level1_max)\n",
    "        out_enc_level1_max = self.encoder_level1_max2(out_dec_level1_max)\n",
    "\n",
    "        inp_enc_level2_max = self.down1_2_max2(out_enc_level1_max)\n",
    "        out_enc_level2_max = self.encoder_level2_max2(inp_enc_level2_max)\n",
    "\n",
    "        inp_enc_level4_max = self.down2_3_max2(out_enc_level2_max)\n",
    "        latent_max = self.latent_max2(inp_enc_level4_max)\n",
    "        BFF_max_2 = latent_max\n",
    "\n",
    "        inp_dec_level2_max = self.up3_2_max2(latent_max)\n",
    "        inp_dec_level2_max = torch.cat([inp_dec_level2_max, out_enc_level2_max], 1)\n",
    "        inp_dec_level2_max = self.reduce_chan_level2_max2(inp_dec_level2_max)\n",
    "        out_dec_level2_max = self.decoder_level2_max2(inp_dec_level2_max)\n",
    "\n",
    "        inp_dec_level1_max = self.up2_1_max2(out_dec_level2_max)\n",
    "        inp_dec_level1_max = torch.cat([inp_dec_level1_max, out_enc_level1_max], 1)\n",
    "        inp_dec_level1_max = self.reduce_chan_level1_max2(inp_dec_level1_max)\n",
    "        out_dec_level1_max = self.decoder_level1_max2(inp_dec_level1_max)\n",
    "\n",
    "        out_dec_level1_max = self.output_max_context2(out_dec_level1_max)\n",
    "        out_enc_level1_max = self.encoder_level1_max3(out_dec_level1_max)\n",
    "\n",
    "        inp_enc_level2_max = self.down1_2_max3(out_enc_level1_max)\n",
    "        out_enc_level2_max = self.encoder_level2_max3(inp_enc_level2_max)\n",
    "\n",
    "        inp_enc_level4_max = self.down2_3_max3(out_enc_level2_max)\n",
    "        latent_max = self.latent_max3(inp_enc_level4_max)\n",
    "        BFF_max_3 = latent_max\n",
    "\n",
    "        BFF1 = self.BF1(BFF_max_1, BFF_max_2)\n",
    "        BFF2 = self.BF2(BFF_max_2, BFF_max_3)\n",
    "\n",
    "        BFF1 = F.interpolate(BFF1, scale_factor=0.5)\n",
    "        BFF2 = F.interpolate(BFF2, scale_factor=0.5)\n",
    "\n",
    "        inp_dec_level2_max = self.up3_2_max3(latent_max)\n",
    "\n",
    "        BFF3_1 = latent_mid\n",
    "        latent_mid = latent_mid + BFF1\n",
    "\n",
    "        inp_dec_level2_mid = self.up3_2_mid(latent_mid)\n",
    "        inp_dec_level2_mid = torch.cat([inp_dec_level2_mid, out_enc_level2_mid], 1)\n",
    "        inp_dec_level2_mid = self.reduce_chan_level2_mid1(inp_dec_level2_mid)\n",
    "        out_dec_level2_mid = self.decoder_level2_mid1(inp_dec_level2_mid)\n",
    "\n",
    "        inp_dec_level1_mid = self.up2_1_mid(out_dec_level2_mid)\n",
    "        inp_dec_level1_mid = torch.cat([inp_dec_level1_mid, out_enc_level1_mid], 1)\n",
    "        inp_dec_level1_mid = self.reduce_chan_level1_mid1(inp_dec_level1_mid)\n",
    "        out_dec_level1_mid = self.decoder_level1_mid1(inp_dec_level1_mid)\n",
    "\n",
    "        out_dec_level1_mid = self.output_mid_context(out_dec_level1_mid)\n",
    "        out_enc_level1_mid = self.encoder_level1_mid2(out_dec_level1_mid)\n",
    "\n",
    "        inp_enc_level2_mid = self.down1_2_mid2(out_enc_level1_mid)\n",
    "        out_enc_level2_mid = self.encoder_level2_mid2(inp_enc_level2_mid)\n",
    "\n",
    "        inp_enc_level4_mid = self.down2_3_mid2(out_enc_level2_mid)\n",
    "        latent_mid = self.latent_mid2(inp_enc_level4_mid)\n",
    "        BFF3_2 = latent_mid\n",
    "        BFF3 = self.BF3(BFF3_1, BFF3_2)\n",
    "        BFF3 = F.interpolate(BFF3, scale_factor=0.5)\n",
    "\n",
    "        latent_mid = latent_mid + BFF2\n",
    "\n",
    "        inp_dec_level2_mid = self.up3_2_mid2(latent_mid)\n",
    "\n",
    "        latent_small = latent_small + BFF3\n",
    "\n",
    "        inp_dec_level2_small = self.up3_2_small(latent_small)\n",
    "        inp_dec_level2_small = torch.cat([inp_dec_level2_small, out_enc_level2_small], 1)\n",
    "        inp_dec_level2_small = self.reduce_chan_level2_small(inp_dec_level2_small)\n",
    "        out_dec_level2_small = self.decoder_level2_small(inp_dec_level2_small)\n",
    "\n",
    "        inp_dec_level1_small = self.up2_1_small(out_dec_level2_small)\n",
    "        inp_dec_level1_small = torch.cat([inp_dec_level1_small, out_enc_level1_small], 1)\n",
    "        inp_dec_level1_small = self.reduce_chan_level1_small(inp_dec_level1_small)\n",
    "        out_dec_level1_small = self.decoder_level1_small(inp_dec_level1_small)\n",
    "\n",
    "        small_2_mid = out_dec_level1_small\n",
    "\n",
    "        out_dec_level1_small = self.output_small(out_dec_level1_small) + inp_img_small\n",
    "\n",
    "        outputs.append(out_dec_level1_small)\n",
    "        small = F.interpolate(out_dec_level1_small, scale_factor=2)\n",
    "\n",
    "        inp_dec_level2_mid = torch.cat([inp_dec_level2_mid, out_enc_level2_mid], 1)\n",
    "        inp_dec_level2_mid = self.reduce_chan_level2_mid2(inp_dec_level2_mid)\n",
    "        out_dec_level2_mid = self.decoder_level2_mid2(inp_dec_level2_mid)\n",
    "\n",
    "        inp_dec_level1_mid = self.up2_1_mid2(out_dec_level2_mid)\n",
    "        inp_dec_level1_mid = torch.cat([inp_dec_level1_mid, out_enc_level1_mid], 1)\n",
    "        inp_dec_level1_mid = self.reduce_chan_level1_mid2(inp_dec_level1_mid)\n",
    "        out_dec_level1_mid = self.decoder_level1_mid2(inp_dec_level1_mid)\n",
    "\n",
    "        small_2_mid = F.interpolate(small_2_mid, scale_factor=2)\n",
    "        out_dec_level1_mid = out_dec_level1_mid + small_2_mid\n",
    "\n",
    "        mid_2_max = out_dec_level1_mid\n",
    "\n",
    "        out_dec_level1_mid = self.output_mid(out_dec_level1_mid) + inp_img_mid\n",
    "\n",
    "        outputs.append(out_dec_level1_mid)\n",
    "        mid = F.interpolate(out_dec_level1_mid, scale_factor=2)\n",
    "\n",
    "        inp_dec_level2_max = torch.cat([inp_dec_level2_max, out_enc_level2_max], 1)\n",
    "        inp_dec_level2_max = self.reduce_chan_level2_max3(inp_dec_level2_max)\n",
    "        out_dec_level2_max = self.decoder_level2_max3(inp_dec_level2_max)\n",
    "\n",
    "        inp_dec_level1_max = self.up2_1_max3(out_dec_level2_max)\n",
    "        inp_dec_level1_max = torch.cat([inp_dec_level1_max, out_enc_level1_max], 1)\n",
    "        inp_dec_level1_max = self.reduce_chan_level1_max2(inp_dec_level1_max)\n",
    "        mid_2_max = F.interpolate(mid_2_max, scale_factor=2)\n",
    "        out_dec_level1_max = self.decoder_level1_max3(inp_dec_level1_max) + mid_2_max\n",
    "\n",
    "        out_dec_level1_max = self.output_max(out_dec_level1_max) + inp_img_max\n",
    "\n",
    "        outputs.append(out_dec_level1_max)\n",
    "\n",
    "        return outputs[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cb36bd",
   "metadata": {
    "papermill": {
     "duration": 0.002693,
     "end_time": "2026-01-01T00:30:46.746085",
     "exception": false,
     "start_time": "2026-01-01T00:30:46.743392",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0d80dac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T00:30:46.752700Z",
     "iopub.status.busy": "2026-01-01T00:30:46.752498Z",
     "iopub.status.idle": "2026-01-01T01:40:38.000564Z",
     "shell.execute_reply": "2026-01-01T01:40:37.999647Z"
    },
    "papermill": {
     "duration": 4191.253122,
     "end_time": "2026-01-01T01:40:38.001954",
     "exception": false,
     "start_time": "2026-01-01T00:30:46.748832",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===> Testing Rain100H\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "100%|██████████| 100/100 [01:30<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===> Testing Rain100L\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:29<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===> Testing Test100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98/98 [01:28<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===> Testing Test1200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [18:07<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===> Testing Test2800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2800/2800 [47:11<00:00,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing completed ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "datasets = [\"Rain100H\", \"Rain100L\", \"Test100\", \"Test1200\", \"Test2800\"]\n",
    "weights = \"/kaggle/input/checkpoints/Deraining/models/Multiscale/model_best.pth\"\n",
    "win_size = 256\n",
    "\n",
    "model_restoration_test = MultiscaleNet()\n",
    "load_checkpoint(model_restoration_test, weights)\n",
    "model_restoration_test.cuda()\n",
    "model_restoration_test = nn.DataParallel(model_restoration_test)\n",
    "model_restoration_test.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for dataset in datasets:\n",
    "        print(f\"\\n===> Testing {dataset}\")\n",
    "\n",
    "        input_dir = f\"/kaggle/input/rain13kdataset/test/test/{dataset}/input/\"\n",
    "        output_dir = f\"/kaggle/working/results/{dataset}/\"\n",
    "        mkdir(output_dir)\n",
    "\n",
    "        test_dataset = DataLoaderTest(input_dir, img_options={})\n",
    "        test_loader = DataLoader(\n",
    "            dataset=test_dataset,\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            drop_last=False,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        for ii, data_test in enumerate(tqdm(test_loader), 0):\n",
    "            torch.cuda.ipc_collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            input_ = data_test[0].cuda()\n",
    "            filenames = data_test[1]\n",
    "\n",
    "            _, _, Hx, Wx = input_.shape\n",
    "            pad_h = (win_size - Hx % win_size) % win_size\n",
    "            pad_w = (win_size - Wx % win_size) % win_size\n",
    "\n",
    "            input_pad = F.pad(input_, (0, pad_w, 0, pad_h), mode='reflect')\n",
    "            input_re, batch_list = window_partitionx(input_pad, win_size)\n",
    "\n",
    "            restored = model_restoration_test(input_re)\n",
    "            restored = window_reversex(\n",
    "                restored[0], win_size, Hx + pad_h, Wx + pad_w, batch_list\n",
    "            )\n",
    "\n",
    "            restored = restored[:, :, :Hx, :Wx]\n",
    "            restored = torch.clamp(restored, 0, 1)\n",
    "            restored = restored.permute(0, 2, 3, 1).cpu().numpy()\n",
    "\n",
    "            for b in range(len(restored)):\n",
    "                restored_img = img_as_ubyte(restored[b])\n",
    "                save_img(os.path.join(output_dir, filenames[b] + \".png\"), restored_img)\n",
    "\n",
    "print(\"=== Testing completed ===\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cbdb1d",
   "metadata": {
    "papermill": {
     "duration": 0.160379,
     "end_time": "2026-01-01T01:40:38.326697",
     "exception": false,
     "start_time": "2026-01-01T01:40:38.166318",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d90c216",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T01:40:38.651874Z",
     "iopub.status.busy": "2026-01-01T01:40:38.651071Z",
     "iopub.status.idle": "2026-01-01T01:40:38.657156Z",
     "shell.execute_reply": "2026-01-01T01:40:38.656449Z"
    },
    "papermill": {
     "duration": 0.171756,
     "end_time": "2026-01-01T01:40:38.658292",
     "exception": false,
     "start_time": "2026-01-01T01:40:38.486536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_gt_file(gt_dir, pred_name):\n",
    "    base = os.path.splitext(pred_name)[0]\n",
    "    for ext in [\"png\", \"jpg\", \"jpeg\"]:\n",
    "        candidate = os.path.join(gt_dir, base + \".\" + ext)\n",
    "        if os.path.exists(candidate):\n",
    "            return candidate\n",
    "    return None\n",
    "\n",
    "def append_results(save_file, dataset_name, psnr, ssim):\n",
    "    header = \"Dataset,PSNR,SSIM\\n\"\n",
    "    if not os.path.exists(save_file):\n",
    "        with open(save_file, \"w\") as f:\n",
    "            f.write(header)\n",
    "    with open(save_file, \"a\") as f:\n",
    "        f.write(f\"{dataset_name},{psnr:.4f},{ssim:.4f}\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c50de421",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T01:40:38.978089Z",
     "iopub.status.busy": "2026-01-01T01:40:38.977459Z",
     "iopub.status.idle": "2026-01-01T01:43:59.973776Z",
     "shell.execute_reply": "2026-01-01T01:43:59.972763Z"
    },
    "papermill": {
     "duration": 201.157792,
     "end_time": "2026-01-01T01:43:59.975133",
     "exception": false,
     "start_time": "2026-01-01T01:40:38.817341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===> Evaluating Rain100H\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:03<00:00, 31.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rain100H | PSNR: 26.0931 | SSIM: 0.7885\n",
      "\n",
      "===> Evaluating Rain100L\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:03<00:00, 33.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rain100L | PSNR: 30.5683 | SSIM: 0.9176\n",
      "\n",
      "===> Evaluating Test100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98/98 [00:03<00:00, 28.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test100 | PSNR: 24.5943 | SSIM: 0.8226\n",
      "\n",
      "===> Evaluating Test1200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [01:19<00:00, 15.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test1200 | PSNR: 30.9124 | SSIM: 0.8868\n",
      "\n",
      "===> Evaluating Test2800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2800/2800 [01:52<00:00, 24.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test2800 | PSNR: 29.9599 | SSIM: 0.8998\n",
      "\n",
      "Saved results to: /kaggle/working/results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "datasets = [\"Rain100H\", \"Rain100L\", \"Test100\", \"Test1200\", \"Test2800\"]\n",
    "save_file = \"/kaggle/working/results.csv\"\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(f\"\\n===> Evaluating {dataset}\")\n",
    "\n",
    "    result_dir = f\"/kaggle/working/results/{dataset}/\"\n",
    "    gt_dir = f\"/kaggle/input/rain13kdataset/test/test/{dataset}/target/\"\n",
    "\n",
    "    result_files = sorted(os.listdir(result_dir))\n",
    "    psnr_list, ssim_list = [], []\n",
    "\n",
    "    for name in tqdm(result_files):\n",
    "        pred_path = os.path.join(result_dir, name)\n",
    "        gt_path = find_gt_file(gt_dir, name)\n",
    "\n",
    "        if gt_path is None:\n",
    "            print(f\"GT not found for {name}\")\n",
    "            continue\n",
    "\n",
    "        pred = Image.open(pred_path).convert(\"L\")\n",
    "        gt = Image.open(gt_path).convert(\"L\")\n",
    "\n",
    "        pred_np = np.array(pred, dtype=np.uint8)\n",
    "        gt_np = np.array(gt, dtype=np.uint8)\n",
    "\n",
    "        psnr_list.append(numpyPSNR(pred_np, gt_np))\n",
    "        ssim_list.append(ssim(gt_np, pred_np, data_range=255))\n",
    "\n",
    "    avg_psnr = np.mean(psnr_list)\n",
    "    avg_ssim = np.mean(ssim_list)\n",
    "\n",
    "    print(f\"{dataset} | PSNR: {avg_psnr:.4f} | SSIM: {avg_ssim:.4f}\")\n",
    "    append_results(save_file, dataset, avg_psnr, avg_ssim)\n",
    "\n",
    "print(\"\\nSaved results to:\", save_file)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 3777739,
     "sourceId": 6534542,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8810955,
     "sourceId": 13930904,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8829332,
     "sourceId": 14354438,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4407.864227,
   "end_time": "2026-01-01T01:44:01.728420",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-01T00:30:33.864193",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
