{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33012c32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T08:40:35.929291Z",
     "iopub.status.busy": "2025-12-31T08:40:35.928679Z",
     "iopub.status.idle": "2025-12-31T08:41:40.896905Z",
     "shell.execute_reply": "2025-12-31T08:41:40.895953Z"
    },
    "papermill": {
     "duration": 64.975088,
     "end_time": "2025-12-31T08:41:40.898601",
     "exception": false,
     "start_time": "2025-12-31T08:40:35.923513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m112.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m97.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install einops natsort kornia -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b99335b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T08:41:40.932881Z",
     "iopub.status.busy": "2025-12-31T08:41:40.932591Z",
     "iopub.status.idle": "2025-12-31T08:42:04.574110Z",
     "shell.execute_reply": "2025-12-31T08:42:04.573283Z"
    },
    "papermill": {
     "duration": 23.659731,
     "end_time": "2025-12-31T08:42:04.575579",
     "exception": false,
     "start_time": "2025-12-31T08:41:40.915848",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-31 08:41:50.001755: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1767170510.203120      20 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1767170510.260404      20 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorboard/compat/__init__.py\u001b[0m in \u001b[0;36mtf\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnotf\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'notf' from 'tensorboard.compat' (/usr/local/lib/python3.11/dist-packages/tensorboard/compat/__init__.py)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorboard/compat/__init__.py\u001b[0m in \u001b[0;36mtf\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnotf\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'notf' from 'tensorboard.compat' (/usr/local/lib/python3.11/dist-packages/tensorboard/compat/__init__.py)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorboard/compat/__init__.py\u001b[0m in \u001b[0;36mtf\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnotf\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'notf' from 'tensorboard.compat' (/usr/local/lib/python3.11/dist-packages/tensorboard/compat/__init__.py)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorboard/compat/__init__.py\u001b[0m in \u001b[0;36mtf\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnotf\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'notf' from 'tensorboard.compat' (/usr/local/lib/python3.11/dist-packages/tensorboard/compat/__init__.py)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorboard/compat/__init__.py\u001b[0m in \u001b[0;36mtf\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnotf\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'notf' from 'tensorboard.compat' (/usr/local/lib/python3.11/dist-packages/tensorboard/compat/__init__.py)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "\n",
    "import torch\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from skimage import img_as_ubyte\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import kornia\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import OrderedDict\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import os\n",
    "\n",
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "torch.cuda.manual_seed_all(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50cb4a27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T08:42:04.610660Z",
     "iopub.status.busy": "2025-12-31T08:42:04.609932Z",
     "iopub.status.idle": "2025-12-31T08:42:04.615185Z",
     "shell.execute_reply": "2025-12-31T08:42:04.614434Z"
    },
    "papermill": {
     "duration": 0.023254,
     "end_time": "2025-12-31T08:42:04.616268",
     "exception": false,
     "start_time": "2025-12-31T08:42:04.593014",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def torchPSNR(tar_img, prd_img):\n",
    "    imdff = torch.clamp(prd_img, 0, 1) - torch.clamp(tar_img, 0, 1)\n",
    "    rmse = (imdff**2).mean().sqrt()\n",
    "    ps = 20 * torch.log10(1 / rmse)\n",
    "    return ps\n",
    "\n",
    "def save_img(filepath, img):\n",
    "    cv2.imwrite(filepath, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "def numpyPSNR(tar_img, prd_img):\n",
    "    imdff = np.float32(prd_img) - np.float32(tar_img)\n",
    "    rmse = np.sqrt(np.mean(imdff**2))\n",
    "    ps = 20 * np.log10(255 / rmse)\n",
    "    return ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "006f8048",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T08:42:04.648920Z",
     "iopub.status.busy": "2025-12-31T08:42:04.648487Z",
     "iopub.status.idle": "2025-12-31T08:42:04.654345Z",
     "shell.execute_reply": "2025-12-31T08:42:04.653811Z"
    },
    "papermill": {
     "duration": 0.023373,
     "end_time": "2025-12-31T08:42:04.655406",
     "exception": false,
     "start_time": "2025-12-31T08:42:04.632033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# đếm số lượng tham số của model\n",
    "def get_parameter_number(net):\n",
    "    total_num = sum(np.prod(p.size()) for p in net.parameters())\n",
    "    trainable_num = sum(np.prod(p.size()) for p in net.parameters() if p.requires_grad)\n",
    "    print('Total: ', total_num)\n",
    "    print('Trainable: ', trainable_num)\n",
    "# load trọng số đã train vào model\n",
    "def load_checkpoint(model, weights):\n",
    "    checkpoint = torch.load(weights, map_location='cpu')\n",
    "    try:\n",
    "        model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    except:\n",
    "        state_dict = checkpoint[\"state_dict\"]\n",
    "        new_state_dict = OrderedDict()\n",
    "        for k, v in state_dict.items():\n",
    "            name = k[7:] if k.startswith('module.') else k\n",
    "            new_state_dict[name] = v\n",
    "        model.load_state_dict(new_state_dict)\n",
    "# biết model đã train đến epoch nào\n",
    "def load_start_epoch(weights):\n",
    "    checkpoint = torch.load(weights)\n",
    "    epoch = checkpoint[\"epoch\"]\n",
    "    return epoch\n",
    "# khôi phục trạng thái optimizer: momentum, adam moments, learning rate history\n",
    "def load_optim(optimizer, weights):\n",
    "    checkpoint = torch.load(weights)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "# tạo thư mục nếu chưa tồn tại\n",
    "def mkdir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dcdd81d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T08:42:04.687793Z",
     "iopub.status.busy": "2025-12-31T08:42:04.687339Z",
     "iopub.status.idle": "2025-12-31T08:42:07.654846Z",
     "shell.execute_reply": "2025-12-31T08:42:07.654234Z"
    },
    "papermill": {
     "duration": 2.985138,
     "end_time": "2025-12-31T08:42:07.656137",
     "exception": false,
     "start_time": "2025-12-31T08:42:04.670999",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in ['jpeg', 'JPEG', 'jpg', 'png', 'JPG', 'PNG', 'gif'])\n",
    "\n",
    "class DataLoaderTest(torch.utils.data.Dataset):\n",
    "    def __init__(self, inp_dir, img_options):\n",
    "        super(DataLoaderTest, self).__init__()\n",
    "        inp_files = sorted(os.listdir(inp_dir))\n",
    "        self.inp_filenames = [os.path.join(inp_dir, x) for x in inp_files if is_image_file(x)]\n",
    "        self.inp_size = len(self.inp_filenames)\n",
    "        self.img_options = img_options\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.inp_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path_inp = self.inp_filenames[index]\n",
    "        filename = os.path.splitext(os.path.split(path_inp)[-1])[0]\n",
    "        inp = Image.open(path_inp)\n",
    "        inp = TF.to_tensor(inp)\n",
    "        return inp, filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77642f76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T08:42:07.690614Z",
     "iopub.status.busy": "2025-12-31T08:42:07.690180Z",
     "iopub.status.idle": "2025-12-31T08:42:07.705493Z",
     "shell.execute_reply": "2025-12-31T08:42:07.704988Z"
    },
    "papermill": {
     "duration": 0.033229,
     "end_time": "2025-12-31T08:42:07.706526",
     "exception": false,
     "start_time": "2025-12-31T08:42:07.673297",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataLoaderTrainCustom(torch.utils.data.Dataset):\n",
    "    def __init__(self, inp_files, tar_files, img_options):\n",
    "        super().__init__()\n",
    "        self.inp_filenames = inp_files\n",
    "        self.tar_filenames = tar_files\n",
    "        self.img_options = img_options\n",
    "        self.sizex = len(self.tar_filenames)\n",
    "        self.ps = self.img_options['patch_size']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.sizex\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index_ = index % self.sizex\n",
    "        ps = self.ps\n",
    "        inp_path = self.inp_filenames[index_]\n",
    "        tar_path = self.tar_filenames[index_]\n",
    "        inp_img = Image.open(inp_path)\n",
    "        tar_img = Image.open(tar_path)\n",
    "        \n",
    "        w, h = tar_img.size\n",
    "        padw = ps - w if w < ps else 0\n",
    "        padh = ps - h if h < ps else 0\n",
    "        if padw != 0 or padh != 0:\n",
    "            inp_img = TF.pad(inp_img, (0, 0, padw, padh), padding_mode='reflect')\n",
    "            tar_img = TF.pad(tar_img, (0, 0, padw, padh), padding_mode='reflect')\n",
    "        \n",
    "        # Data augmentation \n",
    "        aug = random.randint(0, 2)\n",
    "        if aug == 1:\n",
    "            inp_img = TF.adjust_gamma(inp_img, 1)\n",
    "            tar_img = TF.adjust_gamma(tar_img, 1)\n",
    "        \n",
    "        aug = random.randint(0, 2)\n",
    "        if aug == 1:\n",
    "            sat_factor = 1 + (0.2 - 0.4 * np.random.rand())\n",
    "            inp_img = TF.adjust_saturation(inp_img, sat_factor)\n",
    "            tar_img = TF.adjust_saturation(tar_img, sat_factor)\n",
    "        \n",
    "        inp_img = TF.to_tensor(inp_img)\n",
    "        tar_img = TF.to_tensor(tar_img)\n",
    "        \n",
    "        hh, ww = tar_img.shape[1], tar_img.shape[2]\n",
    "        rr = random.randint(0, hh - ps)\n",
    "        cc = random.randint(0, ww - ps)\n",
    "        \n",
    "        aug = random.randint(0, 8)\n",
    "        inp_img = inp_img[:, rr:rr+ps, cc:cc+ps]\n",
    "        tar_img = tar_img[:, rr:rr+ps, cc:cc+ps]\n",
    "        \n",
    "        # Augmentation\n",
    "        if aug == 1:\n",
    "            inp_img = inp_img.flip(1)\n",
    "            tar_img = tar_img.flip(1)\n",
    "        elif aug == 2:\n",
    "            inp_img = inp_img.flip(2)\n",
    "            tar_img = tar_img.flip(2)\n",
    "        elif aug == 3:\n",
    "            inp_img = torch.rot90(inp_img, dims=(1, 2))\n",
    "            tar_img = torch.rot90(tar_img, dims=(1, 2))\n",
    "        elif aug == 4:\n",
    "            inp_img = torch.rot90(inp_img, dims=(1, 2), k=2)\n",
    "            tar_img = torch.rot90(tar_img, dims=(1, 2), k=2)\n",
    "        elif aug == 5:\n",
    "            inp_img = torch.rot90(inp_img, dims=(1, 2), k=3)\n",
    "            tar_img = torch.rot90(tar_img, dims=(1, 2), k=3)\n",
    "        elif aug == 6:\n",
    "            inp_img = torch.rot90(inp_img.flip(1), dims=(1, 2))\n",
    "            tar_img = torch.rot90(tar_img.flip(1), dims=(1, 2))\n",
    "        elif aug == 7:\n",
    "            inp_img = torch.rot90(inp_img.flip(2), dims=(1, 2))\n",
    "            tar_img = torch.rot90(tar_img.flip(2), dims=(1, 2))\n",
    "        \n",
    "        filename = os.path.splitext(os.path.split(tar_path)[-1])[0]\n",
    "        return tar_img, inp_img, filename\n",
    "\n",
    "class DataLoaderValCustom(torch.utils.data.Dataset):\n",
    "    def __init__(self, inp_files, tar_files, img_options):\n",
    "        super().__init__()\n",
    "        self.inp_filenames = inp_files\n",
    "        self.tar_filenames = tar_files\n",
    "        self.img_options = img_options\n",
    "        self.sizex = len(self.tar_filenames)\n",
    "        self.ps = self.img_options['patch_size']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.sizex\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index_ = index % self.sizex\n",
    "        ps = self.ps\n",
    "        inp_path = self.inp_filenames[index_]\n",
    "        tar_path = self.tar_filenames[index_]\n",
    "        inp_img = Image.open(inp_path)\n",
    "        tar_img = Image.open(tar_path)\n",
    "        \n",
    "        if self.ps is not None:\n",
    "            inp_img = TF.center_crop(inp_img, (ps, ps))\n",
    "            tar_img = TF.center_crop(tar_img, (ps, ps))\n",
    "        \n",
    "        inp_img = TF.to_tensor(inp_img)\n",
    "        tar_img = TF.to_tensor(tar_img)\n",
    "        filename = os.path.splitext(os.path.split(tar_path)[-1])[0]\n",
    "        return tar_img, inp_img, filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bf1f3f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T08:42:07.739770Z",
     "iopub.status.busy": "2025-12-31T08:42:07.739560Z",
     "iopub.status.idle": "2025-12-31T08:42:07.752234Z",
     "shell.execute_reply": "2025-12-31T08:42:07.751699Z"
    },
    "papermill": {
     "duration": 0.030846,
     "end_time": "2025-12-31T08:42:07.753213",
     "exception": false,
     "start_time": "2025-12-31T08:42:07.722367",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def window_partitions(x, window_size):\n",
    "    if isinstance(window_size, int):\n",
    "        window_size = [window_size, window_size]\n",
    "    B, C, H, W = x.shape\n",
    "    x = x.view(B, C, H // window_size[0], window_size[0], W // window_size[1], window_size[1])\n",
    "    windows = x.permute(0, 2, 4, 1, 3, 5).contiguous().view(-1, C, window_size[0], window_size[1])\n",
    "    return windows\n",
    "\n",
    "def window_reverses(windows, window_size, H, W):\n",
    "    if isinstance(window_size, int):\n",
    "        window_size = [window_size, window_size]\n",
    "    C = windows.shape[1]\n",
    "    x = windows.view(-1, H // window_size[0], W // window_size[1], C, window_size[0], window_size[1])\n",
    "    x = x.permute(0, 3, 1, 4, 2, 5).contiguous().view(-1, C, H, W)\n",
    "    return x\n",
    "\n",
    "def window_partitionx(x, window_size):\n",
    "    _, _, H, W = x.shape\n",
    "    h, w = window_size * (H // window_size), window_size * (W // window_size)\n",
    "    x_main = window_partitions(x[:, :, :h, :w], window_size)\n",
    "    b_main = x_main.shape[0]\n",
    "    if h == H and w == W:\n",
    "        return x_main, [b_main]\n",
    "    if h != H and w != W:\n",
    "        x_r = window_partitions(x[:, :, :h, -window_size:], window_size)\n",
    "        b_r = x_r.shape[0] + b_main\n",
    "        x_d = window_partitions(x[:, :, -window_size:, :w], window_size)\n",
    "        b_d = x_d.shape[0] + b_r\n",
    "        x_dd = x[:, :, -window_size:, -window_size:]\n",
    "        b_dd = x_dd.shape[0] + b_d\n",
    "        return torch.cat([x_main, x_r, x_d, x_dd], dim=0), [b_main, b_r, b_d, b_dd]\n",
    "    if h == H and w != W:\n",
    "        x_r = window_partitions(x[:, :, :h, -window_size:], window_size)\n",
    "        b_r = x_r.shape[0] + b_main\n",
    "        return torch.cat([x_main, x_r], dim=0), [b_main, b_r]\n",
    "    if h != H and w == W:\n",
    "        x_d = window_partitions(x[:, :, -window_size:, :w], window_size)\n",
    "        b_d = x_d.shape[0] + b_main\n",
    "        return torch.cat([x_main, x_d], dim=0), [b_main, b_d]\n",
    "\n",
    "def window_reversex(windows, window_size, H, W, batch_list):\n",
    "    h, w = window_size * (H // window_size), window_size * (W // window_size)\n",
    "    x_main = window_reverses(windows[:batch_list[0], ...], window_size, h, w)\n",
    "    B, C, _, _ = x_main.shape\n",
    "    if torch.is_complex(windows):\n",
    "        res = torch.complex(torch.zeros([B, C, H, W]), torch.zeros([B, C, H, W]))\n",
    "        res = res.to(windows.device)\n",
    "    else:\n",
    "        res = torch.zeros([B, C, H, W], device=windows.device)\n",
    "    res[:, :, :h, :w] = x_main\n",
    "    if h == H and w == W:\n",
    "        return res\n",
    "    if h != H and w != W and len(batch_list) == 4:\n",
    "        x_dd = window_reverses(windows[batch_list[2]:, ...], window_size, window_size, window_size)\n",
    "        res[:, :, h:, w:] = x_dd[:, :, h - H:, w - W:]\n",
    "        x_r = window_reverses(windows[batch_list[0]:batch_list[1], ...], window_size, h, window_size)\n",
    "        res[:, :, :h, w:] = x_r[:, :, :, w - W:]\n",
    "        x_d = window_reverses(windows[batch_list[1]:batch_list[2], ...], window_size, window_size, w)\n",
    "        res[:, :, h:, :w] = x_d[:, :, h - H:, :]\n",
    "        return res\n",
    "    if w != W and len(batch_list) == 2:\n",
    "        x_r = window_reverses(windows[batch_list[0]:batch_list[1], ...], window_size, h, window_size)\n",
    "        res[:, :, :h, w:] = x_r[:, :, :, w - W:]\n",
    "    if h != H and len(batch_list) == 2:\n",
    "        x_d = window_reverses(windows[batch_list[0]:batch_list[1], ...], window_size, window_size, w)\n",
    "        res[:, :, h:, :w] = x_d[:, :, h - H:, :]\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da82f9ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T08:42:07.786546Z",
     "iopub.status.busy": "2025-12-31T08:42:07.786327Z",
     "iopub.status.idle": "2025-12-31T08:42:07.804423Z",
     "shell.execute_reply": "2025-12-31T08:42:07.803793Z"
    },
    "papermill": {
     "duration": 0.035827,
     "end_time": "2025-12-31T08:42:07.805406",
     "exception": false,
     "start_time": "2025-12-31T08:42:07.769579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "\n",
    "hidden_list = [256, 256, 256]\n",
    "L = 4\n",
    "\n",
    "def make_coord(shape, ranges=None, flatten=True):\n",
    "    coord_seqs = []\n",
    "    for i, n in enumerate(shape):\n",
    "        if ranges is None:\n",
    "            v0, v1 = -1, 1\n",
    "        else:\n",
    "            v0, v1 = ranges[i]\n",
    "        r = (v1 - v0) / (2 * n)\n",
    "        seq = v0 + r + (2 * r) * torch.arange(n).float()\n",
    "        coord_seqs.append(seq)\n",
    "    ret = torch.stack(torch.meshgrid(*coord_seqs), dim=-1)\n",
    "    if flatten:\n",
    "        ret = ret.view(-1, ret.shape[-1])\n",
    "    return ret\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hidden_list):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        lastv = in_dim\n",
    "        for hidden in hidden_list:\n",
    "            layers.append(nn.Linear(lastv, hidden))\n",
    "            layers.append(nn.ReLU())\n",
    "            lastv = hidden\n",
    "        layers.append(nn.Linear(lastv, out_dim))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shape = x.shape[:-1]\n",
    "        x = self.layers(x.view(-1, x.shape[-1]))\n",
    "        return x.view(*shape, -1)\n",
    "\n",
    "class INR(nn.Module):\n",
    "    def __init__(self, dim, local_ensemble=True, feat_unfold=True, cell_decode=True):\n",
    "        super().__init__()\n",
    "        self.local_ensemble = local_ensemble\n",
    "        self.feat_unfold = feat_unfold\n",
    "        self.cell_decode = cell_decode\n",
    "        imnet_in_dim = dim\n",
    "\n",
    "        if self.feat_unfold:\n",
    "            imnet_in_dim *= 9\n",
    "        imnet_in_dim += 2 + 4 * L  \n",
    "        if self.cell_decode:\n",
    "            imnet_in_dim += 2\n",
    "\n",
    "        self.imnet = MLP(imnet_in_dim, 3, hidden_list)\n",
    "\n",
    "    def query_rgb(self, inp, coord, cell=None):\n",
    "        feat = inp\n",
    "        if self.feat_unfold:\n",
    "            feat = F.unfold(feat, 3, padding=1).view(\n",
    "                feat.shape[0], feat.shape[1] * 9, feat.shape[2], feat.shape[3])\n",
    "\n",
    "        if self.local_ensemble:\n",
    "            vx_lst = [-1, 1]\n",
    "            vy_lst = [-1, 1]\n",
    "            eps_shift = 1e-6\n",
    "        else:\n",
    "            vx_lst, vy_lst, eps_shift = [0], [0], 0\n",
    "\n",
    "        rx = 2 / feat.shape[-2] / 2\n",
    "        ry = 2 / feat.shape[-1] / 2\n",
    "        \n",
    "        device = inp.device\n",
    "        feat_coord = make_coord(feat.shape[-2:], flatten=False).to(device) \\\n",
    "            .permute(2, 0, 1) \\\n",
    "            .unsqueeze(0).expand(feat.shape[0], 2, *feat.shape[-2:])\n",
    "\n",
    "        preds = []\n",
    "        areas = []\n",
    "        for vx in vx_lst:\n",
    "            for vy in vy_lst:\n",
    "                coord_ = coord.clone()\n",
    "                coord_[:, :, 0] += vx * rx + eps_shift\n",
    "                coord_[:, :, 1] += vy * ry + eps_shift\n",
    "                coord_.clamp_(-1 + 1e-6, 1 - 1e-6)\n",
    "\n",
    "                bs, q, h, w = feat.shape\n",
    "                q_feat = feat.view(bs, q, -1).permute(0, 2, 1)\n",
    "\n",
    "                bs, q, h, w = feat_coord.shape\n",
    "                q_coord = feat_coord.view(bs, q, -1).permute(0, 2, 1)\n",
    "\n",
    "                points_enc = self.positional_encoding(q_coord, L=L)\n",
    "                q_coord = torch.cat([q_coord, points_enc], dim=-1)  \n",
    "\n",
    "                rel_coord = coord - q_coord\n",
    "                rel_coord[:, :, 0] *= feat.shape[-2]\n",
    "                rel_coord[:, :, 1] *= feat.shape[-1]\n",
    "                inp = torch.cat([q_feat, rel_coord], dim=-1)\n",
    "\n",
    "                if self.cell_decode:\n",
    "                    rel_cell = cell.clone()\n",
    "                    rel_cell[:, :, 0] *= feat.shape[-2]\n",
    "                    rel_cell[:, :, 1] *= feat.shape[-1]\n",
    "                    inp = torch.cat([inp, rel_cell], dim=-1)\n",
    "\n",
    "                bs, q = coord.shape[:2]\n",
    "                pred = self.imnet(inp.view(bs * q, -1)).view(bs, q, -1)\n",
    "                preds.append(pred)\n",
    "\n",
    "                area = torch.abs(rel_coord[:, :, 0] * rel_coord[:, :, 1])\n",
    "                areas.append(area + 1e-9)\n",
    "\n",
    "        tot_area = torch.stack(areas).sum(dim=0)\n",
    "        if self.local_ensemble:\n",
    "            t = areas[0];\n",
    "            areas[0] = areas[3];\n",
    "            areas[3] = t\n",
    "            t = areas[1];\n",
    "            areas[1] = areas[2];\n",
    "            areas[2] = t\n",
    "        ret = 0\n",
    "        for pred, area in zip(preds, areas):\n",
    "            ret = ret + pred * (area / tot_area).unsqueeze(-1)\n",
    "\n",
    "        bs, q, h, w = feat.shape\n",
    "        ret = ret.view(bs, h, w, -1).permute(0, 3, 1, 2)\n",
    "        return ret\n",
    "\n",
    "    def forward(self, inp):\n",
    "        h, w = inp.shape[2], inp.shape[3]\n",
    "        B = inp.shape[0]\n",
    "        #coord = make_coord((h, w)).cuda() # cũ\n",
    "        device = inp.device\n",
    "        coord = make_coord((h, w)).to(device)\n",
    "        cell = torch.ones_like(coord)\n",
    "        cell[:, 0] *= 2 / h\n",
    "        cell[:, 1] *= 2 / w\n",
    "        cell = cell.unsqueeze(0).repeat(B, 1, 1)\n",
    "        coord = coord.unsqueeze(0).repeat(B, 1, 1)\n",
    "        points_enc = self.positional_encoding(coord, L=L)\n",
    "        coord = torch.cat([coord, points_enc], dim=-1)  \n",
    "\n",
    "        return self.query_rgb(inp, coord, cell)\n",
    "\n",
    "    def positional_encoding(self, input, L): \n",
    "        shape = input.shape\n",
    "        device = input.device\n",
    "        freq = 2 ** torch.arange(L, dtype=torch.float32).to(device) * np.pi  \n",
    "        spectrum = input[..., None] * freq  \n",
    "        sin, cos = spectrum.sin(), spectrum.cos()  \n",
    "        input_enc = torch.stack([sin, cos], dim=-2)  \n",
    "        input_enc = input_enc.view(*shape[:-1], -1)  \n",
    "\n",
    "        return input_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2aeb1c96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T08:42:07.839008Z",
     "iopub.status.busy": "2025-12-31T08:42:07.838804Z",
     "iopub.status.idle": "2025-12-31T08:42:07.879289Z",
     "shell.execute_reply": "2025-12-31T08:42:07.878741Z"
    },
    "papermill": {
     "duration": 0.059418,
     "end_time": "2025-12-31T08:42:07.880841",
     "exception": false,
     "start_time": "2025-12-31T08:42:07.821423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import numbers\n",
    "from einops import rearrange\n",
    "#from mlp import INR\n",
    "\n",
    "def to_3d(x):\n",
    "    return rearrange(x, 'b c h w -> b (h w) c')\n",
    "\n",
    "\n",
    "def to_4d(x, h, w):\n",
    "    return rearrange(x, 'b (h w) c -> b c h w', h=h, w=w)\n",
    "\n",
    "\n",
    "class BasicConv(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, kernel_size, stride, bias=False, norm=False, relu=True, transpose=False,\n",
    "                 channel_shuffle_g=0, norm_method=nn.BatchNorm2d, groups=1):\n",
    "        super(BasicConv, self).__init__()\n",
    "        self.channel_shuffle_g = channel_shuffle_g\n",
    "        self.norm = norm\n",
    "        if bias and norm:\n",
    "            bias = False\n",
    "\n",
    "        padding = kernel_size // 2\n",
    "        layers = list()\n",
    "        if transpose:\n",
    "            padding = kernel_size // 2 - 1\n",
    "            layers.append(\n",
    "                nn.ConvTranspose2d(in_channel, out_channel, kernel_size, padding=padding, stride=stride, bias=bias,\n",
    "                                   groups=groups))\n",
    "        else:\n",
    "            layers.append(\n",
    "                nn.Conv2d(in_channel, out_channel, kernel_size, padding=padding, stride=stride, bias=bias,\n",
    "                          groups=groups))\n",
    "        if norm:\n",
    "            layers.append(norm_method(out_channel))\n",
    "        elif relu:\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "\n",
    "        self.main = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "\n",
    "\n",
    "class BiasFree_LayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape):\n",
    "        super(BiasFree_LayerNorm, self).__init__()\n",
    "        if isinstance(normalized_shape, numbers.Integral):\n",
    "            normalized_shape = (normalized_shape,)\n",
    "        normalized_shape = torch.Size(normalized_shape)\n",
    "\n",
    "        assert len(normalized_shape) == 1\n",
    "\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.normalized_shape = normalized_shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        sigma = x.var(-1, keepdim=True, unbiased=False)\n",
    "        return x / torch.sqrt(sigma + 1e-5) * self.weight\n",
    "\n",
    "\n",
    "class WithBias_LayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape):\n",
    "        super(WithBias_LayerNorm, self).__init__()\n",
    "        if isinstance(normalized_shape, numbers.Integral):\n",
    "            normalized_shape = (normalized_shape,)\n",
    "        normalized_shape = torch.Size(normalized_shape)\n",
    "\n",
    "        assert len(normalized_shape) == 1\n",
    "\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
    "        self.normalized_shape = normalized_shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu = x.mean(-1, keepdim=True)\n",
    "        sigma = x.var(-1, keepdim=True, unbiased=False)\n",
    "        return (x - mu) / torch.sqrt(sigma + 1e-5) * self.weight + self.bias\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim, LayerNorm_type):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        if LayerNorm_type == 'BiasFree':\n",
    "            self.body = BiasFree_LayerNorm(dim)\n",
    "        else:\n",
    "            self.body = WithBias_LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, w = x.shape[-2:]\n",
    "        return to_4d(self.body(to_3d(x)), h, w)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, ffn_expansion_factor, bias, BasicConv=BasicConv):\n",
    "        super(FeedForward, self).__init__()\n",
    "\n",
    "        hidden_features = int(dim * ffn_expansion_factor)\n",
    "\n",
    "        self.project_in = nn.Conv2d(dim, hidden_features * 2, kernel_size=1, bias=bias)\n",
    "\n",
    "        self.dwconv = BasicConv(hidden_features * 2, hidden_features * 2, kernel_size=3, stride=1, bias=bias,\n",
    "                                relu=False, groups=hidden_features * 2)\n",
    "\n",
    "        self.project_out = nn.Conv2d(hidden_features, dim, kernel_size=1, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.project_in(x)\n",
    "        x1, x2 = self.dwconv(x).chunk(2, dim=1)\n",
    "        x = F.gelu(x1) * x2\n",
    "        x = self.project_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads, bias, BasicConv=BasicConv):\n",
    "        super(Attention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))\n",
    "\n",
    "        self.qkv = nn.Conv2d(dim, dim * 3, kernel_size=1, bias=bias)\n",
    "        self.qkv_dwconv = BasicConv(dim * 3, dim * 3, kernel_size=3, stride=1, bias=bias, relu=False, groups=dim * 3)\n",
    "        self.project_out = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "\n",
    "        qkv = self.qkv_dwconv(self.qkv(x))\n",
    "        q, k, v = qkv.chunk(3, dim=1)\n",
    "\n",
    "        q = rearrange(q, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
    "        k = rearrange(k, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
    "        v = rearrange(v, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
    "\n",
    "        q = torch.nn.functional.normalize(q, dim=-1)\n",
    "        k = torch.nn.functional.normalize(k, dim=-1)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.temperature\n",
    "        attn = attn.softmax(dim=-1)\n",
    "\n",
    "        out = (attn @ v)\n",
    "\n",
    "        out = rearrange(out, 'b head c (h w) -> b (head c) h w', head=self.num_heads, h=h, w=w)\n",
    "\n",
    "        out = self.project_out(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, ffn_expansion_factor, bias, LayerNorm_type, BasicConv=BasicConv):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        self.norm1 = LayerNorm(dim, LayerNorm_type)\n",
    "        self.attn = Attention(dim, num_heads, bias, BasicConv=BasicConv)\n",
    "        self.norm2 = LayerNorm(dim, LayerNorm_type)\n",
    "        self.ffn = FeedForward(dim, ffn_expansion_factor, bias, BasicConv=BasicConv)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class OverlapPatchEmbed(nn.Module):\n",
    "    def __init__(self, in_c=3, embed_dim=48, bias=False):\n",
    "        super(OverlapPatchEmbed, self).__init__()\n",
    "\n",
    "        self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=3, stride=1, padding=1, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Downsample(nn.Module):\n",
    "    def __init__(self, n_feat):\n",
    "        super(Downsample, self).__init__()\n",
    "\n",
    "        self.body = nn.Sequential(nn.Conv2d(n_feat, n_feat // 2, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                                  nn.PixelUnshuffle(2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.body(x)\n",
    "\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    def __init__(self, n_feat):\n",
    "        super(Upsample, self).__init__()\n",
    "\n",
    "        self.body = nn.Sequential(nn.Conv2d(n_feat, n_feat * 2, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                                  nn.PixelShuffle(2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.body(x)\n",
    "\n",
    "\n",
    "class Fusion(nn.Module):\n",
    "    def __init__(self, in_dim=32):\n",
    "        super(Fusion, self).__init__()\n",
    "        self.chanel_in = in_dim\n",
    "\n",
    "        self.query_conv = nn.Conv2d(in_dim, in_dim, 3, 1, 1, bias=True)\n",
    "        self.key_conv = nn.Conv2d(in_dim, in_dim, 3, 1, 1, bias=True)\n",
    "\n",
    "        self.gamma1 = nn.Conv2d(in_dim * 2, 2, 3, 1, 1, bias=True)\n",
    "        self.gamma2 = nn.Conv2d(in_dim * 2, 2, 3, 1, 1, bias=True)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x_q = self.query_conv(x)\n",
    "        y_k = self.key_conv(y)\n",
    "        energy = x_q * y_k\n",
    "        attention = self.sig(energy)\n",
    "        attention_x = x * attention\n",
    "        attention_y = y * attention\n",
    "\n",
    "        x_gamma = self.gamma1(torch.cat((x, attention_x), dim=1))\n",
    "        x_out = x * x_gamma[:, [0], :, :] + attention_x * x_gamma[:, [1], :, :]\n",
    "\n",
    "        y_gamma = self.gamma2(torch.cat((y, attention_y), dim=1))\n",
    "        y_out = y * y_gamma[:, [0], :, :] + attention_y * y_gamma[:, [1], :, :]\n",
    "\n",
    "        x_s = x_out + y_out\n",
    "\n",
    "        return x_s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0919772e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T08:42:07.918334Z",
     "iopub.status.busy": "2025-12-31T08:42:07.917988Z",
     "iopub.status.idle": "2025-12-31T08:42:08.264172Z",
     "shell.execute_reply": "2025-12-31T08:42:08.263519Z"
    },
    "papermill": {
     "duration": 0.367783,
     "end_time": "2025-12-31T08:42:08.265543",
     "exception": false,
     "start_time": "2025-12-31T08:42:07.897760",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiscaleNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 inp_channels=3,\n",
    "                 out_channels=3,\n",
    "                 dim=48,\n",
    "                 num_blocks=[2, 3, 3],\n",
    "                 heads=[1, 2, 4],\n",
    "                 ffn_expansion_factor=2.66,\n",
    "                 bias=False,\n",
    "                 LayerNorm_type='WithBias',\n",
    "                 ):\n",
    "        super(MultiscaleNet, self).__init__()\n",
    "        self.patch_embed_small = OverlapPatchEmbed(inp_channels, dim)\n",
    "\n",
    "        self.encoder_level1_small = nn.Sequential(*[\n",
    "            TransformerBlock(dim=dim, num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias,\n",
    "                             LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n",
    "\n",
    "        self.down1_2_small = Downsample(dim)\n",
    "        self.encoder_level2_small = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 2 ** 1), num_heads=heads[1], ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[1])])\n",
    "\n",
    "        self.down2_3_small = Downsample(int(dim * 2 ** 1))\n",
    "        self.latent_small = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 2 ** 2), num_heads=heads[2], ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[2])])\n",
    "\n",
    "        self.up3_2_small = Upsample(int(dim * 2 ** 2))\n",
    "        self.reduce_chan_level2_small = nn.Conv2d(int(dim * 2 ** 2), int(dim * 2 ** 1), kernel_size=1, bias=bias)\n",
    "        self.decoder_level2_small = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 2 ** 1), num_heads=heads[1], ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[1])])\n",
    "\n",
    "        self.up2_1_small = Upsample(int(dim * 2 ** 1))\n",
    "        self.reduce_chan_level1_small = nn.Conv2d(int(dim * 2 ** 1), int(dim * 1 ** 1), kernel_size=1, bias=bias)\n",
    "        self.decoder_level1_small = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 1 ** 1), num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n",
    "\n",
    "        self.output_small = nn.Conv2d(int(dim * 1 ** 1), out_channels, kernel_size=3, stride=1, padding=1, bias=bias)\n",
    "\n",
    "        #self.INR = INR(dim).cuda() # cũ\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.INR = INR(dim).to(device) # mới\n",
    "\n",
    "        self.patch_embed_mid = OverlapPatchEmbed(inp_channels, dim)\n",
    "\n",
    "        self.encoder_level1_mid1 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=dim, num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias,\n",
    "                             LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n",
    "\n",
    "        self.encoder_level1_mid2 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=dim, num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias,\n",
    "                             LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n",
    "\n",
    "        self.down1_2_mid = Downsample(dim)\n",
    "        self.down1_2_mid2 = Downsample(dim)\n",
    "        self.encoder_level2_mid1 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 2 ** 1), num_heads=heads[1], ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[1])])\n",
    "        self.encoder_level2_mid2 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 2 ** 1), num_heads=heads[1], ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[1])])\n",
    "\n",
    "        self.down2_3_mid = Downsample(int(dim * 2 ** 1))\n",
    "        self.down2_3_mid2 = Downsample(int(dim * 2 ** 1))\n",
    "        self.latent_mid1 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 2 ** 2), num_heads=heads[2], ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[2])])\n",
    "        self.latent_mid2 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 2 ** 2), num_heads=heads[2], ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[2])])\n",
    "\n",
    "        self.up3_2_mid = Upsample(int(dim * 2 ** 2))\n",
    "        self.up3_2_mid2 = Upsample(int(dim * 2 ** 2))\n",
    "        self.reduce_chan_level2_mid1 = nn.Conv2d(int(dim * 2 ** 2), int(dim * 2 ** 1), kernel_size=1, bias=bias)\n",
    "        self.reduce_chan_level2_mid2 = nn.Conv2d(int(dim * 2 ** 2), int(dim * 2 ** 1), kernel_size=1, bias=bias)\n",
    "        self.decoder_level2_mid1 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 2 ** 1), num_heads=heads[1], ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[1])])\n",
    "        self.decoder_level2_mid2 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 2 ** 1), num_heads=heads[1], ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[1])])\n",
    "\n",
    "        self.up2_1_mid = Upsample(int(dim * 2 ** 1))\n",
    "        self.up2_1_mid2 = Upsample(int(dim * 2 ** 1))\n",
    "        self.reduce_chan_level1_mid1 = nn.Conv2d(int(dim * 2 ** 1), int(dim * 1 ** 1), kernel_size=1, bias=bias)\n",
    "        self.reduce_chan_level1_mid2 = nn.Conv2d(int(dim * 2 ** 1), int(dim * 1 ** 1), kernel_size=1, bias=bias)\n",
    "        self.decoder_level1_mid1 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 1 ** 1), num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n",
    "        self.decoder_level1_mid2 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 1 ** 1), num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n",
    "\n",
    "        self.output_mid = nn.Conv2d(int(dim * 1 ** 1), out_channels, kernel_size=3, stride=1, padding=1, bias=bias)\n",
    "        self.output_mid_context = nn.Conv2d(int(dim * 1 ** 1), dim, kernel_size=3, stride=1, padding=1, bias=bias)\n",
    "\n",
    "        #self.INR2 = INR(dim).cuda() # cũ\n",
    "\n",
    "        self.INR2 = INR(dim).to(device) # mới\n",
    "\n",
    "        self.patch_embed_max = OverlapPatchEmbed(inp_channels, dim)\n",
    "\n",
    "        self.encoder_level1_max1 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=dim, num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias,\n",
    "                             LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n",
    "        self.encoder_level1_max2 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=dim, num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias,\n",
    "                             LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n",
    "        self.encoder_level1_max3 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=dim, num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias,\n",
    "                             LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n",
    "\n",
    "        self.down1_2_max = Downsample(dim)\n",
    "        self.down1_2_max2 = Downsample(dim)\n",
    "        self.down1_2_max3 = Downsample(dim)\n",
    "        self.encoder_level2_max1 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 2 ** 1), num_heads=heads[1], ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[1])])\n",
    "        self.encoder_level2_max2 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 2 ** 1), num_heads=heads[1], ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[1])])\n",
    "        self.encoder_level2_max3 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 2 ** 1), num_heads=heads[1], ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[1])])\n",
    "\n",
    "        self.down2_3_max = Downsample(int(dim * 2 ** 1))\n",
    "        self.down2_3_max2 = Downsample(int(dim * 2 ** 1))\n",
    "        self.down2_3_max3 = Downsample(int(dim * 2 ** 1))\n",
    "        self.latent_max1 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 2 ** 2), num_heads=heads[2], ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[2])])\n",
    "        self.latent_max2 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 2 ** 2), num_heads=heads[2], ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[2])])\n",
    "        self.latent_max3 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 2 ** 2), num_heads=heads[2], ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[2])])\n",
    "\n",
    "        self.up3_2_max = Upsample(int(dim * 2 ** 2))\n",
    "        self.up3_2_max2 = Upsample(int(dim * 2 ** 2))\n",
    "        self.up3_2_max3 = Upsample(int(dim * 2 ** 2))\n",
    "        self.reduce_chan_level2_max1 = nn.Conv2d(int(dim * 2 ** 2), int(dim * 2 ** 1), kernel_size=1, bias=bias)\n",
    "        self.reduce_chan_level2_max2 = nn.Conv2d(int(dim * 2 ** 2), int(dim * 2 ** 1), kernel_size=1, bias=bias)\n",
    "        self.reduce_chan_level2_max3 = nn.Conv2d(int(dim * 2 ** 2), int(dim * 2 ** 1), kernel_size=1, bias=bias)\n",
    "        self.decoder_level2_max1 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 2 ** 1), num_heads=heads[1], ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[1])])\n",
    "        self.decoder_level2_max2 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 2 ** 1), num_heads=heads[1], ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[1])])\n",
    "        self.decoder_level2_max3 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 2 ** 1), num_heads=heads[1], ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[1])])\n",
    "\n",
    "        self.up2_1_max = Upsample(int(dim * 2 ** 1))\n",
    "        self.up2_1_max2 = Upsample(int(dim * 2 ** 1))\n",
    "        self.up2_1_max3 = Upsample(int(dim * 2 ** 1))\n",
    "        self.reduce_chan_level1_max1 = nn.Conv2d(int(dim * 2 ** 1), int(dim * 1 ** 1), kernel_size=1, bias=bias)\n",
    "        self.reduce_chan_level1_max2 = nn.Conv2d(int(dim * 2 ** 1), int(dim * 1 ** 1), kernel_size=1, bias=bias)\n",
    "        self.reduce_chan_level1_max3 = nn.Conv2d(int(dim * 2 ** 1), int(dim * 1 ** 1), kernel_size=1, bias=bias)\n",
    "        self.decoder_level1_max1 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 1 ** 1), num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n",
    "        self.decoder_level1_max2 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 1 ** 1), num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n",
    "        self.decoder_level1_max3 = nn.Sequential(*[\n",
    "            TransformerBlock(dim=int(dim * 1 ** 1), num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor,\n",
    "                             bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n",
    "\n",
    "        self.output_max = nn.Conv2d(int(dim * 1 ** 1), out_channels, kernel_size=3, stride=1, padding=1, bias=bias)\n",
    "        self.output_max_context1 = nn.Conv2d(int(dim * 1 ** 1), dim, kernel_size=3, stride=1, padding=1, bias=bias)\n",
    "        self.output_max_context2 = nn.Conv2d(int(dim * 1 ** 1), dim, kernel_size=3, stride=1, padding=1, bias=bias)\n",
    "\n",
    "        self.BF1 = Fusion(dim * 4)\n",
    "        self.BF2 = Fusion(dim * 4)\n",
    "        self.BF3 = Fusion(dim * 4)\n",
    "\n",
    "        self.upsmall2mid1 = Upsample(int(dim * 4 ** 1))\n",
    "        self.upsmall2mid2 = Upsample(int(dim * 2 ** 1))\n",
    "\n",
    "        self.upmid2max1 = Upsample(int(dim * 4 ** 1))\n",
    "        self.upmid2max2 = Upsample(int(dim * 2 ** 1))\n",
    "\n",
    "    def forward(self, inp_img):\n",
    "        outputs = list()\n",
    "\n",
    "        inp_img_max = inp_img\n",
    "        inp_img_mid = F.interpolate(inp_img, scale_factor=0.5)\n",
    "        inp_img_small = F.interpolate(inp_img, scale_factor=0.25)\n",
    "\n",
    "        inp_enc_level1_small = self.patch_embed_small(inp_img_small)\n",
    "        out_enc_level1_small = self.encoder_level1_small(inp_enc_level1_small)\n",
    "\n",
    "        inp_enc_level2_small = self.down1_2_small(out_enc_level1_small)\n",
    "        out_enc_level2_small = self.encoder_level2_small(inp_enc_level2_small)\n",
    "\n",
    "        inp_enc_level4_small = self.down2_3_small(out_enc_level2_small)\n",
    "        latent_small = self.latent_small(inp_enc_level4_small)\n",
    "        latent_small_mid = self.upsmall2mid1(latent_small)\n",
    "        latent_small_mid = self.upsmall2mid2(latent_small_mid)\n",
    "\n",
    "        outputs.append(inp_img_small)\n",
    "        INR = self.INR(latent_small_mid)\n",
    "        inp_img_small_ = INR + inp_img_small\n",
    "        outputs.append(inp_img_small_)\n",
    "\n",
    "        inp_img_small_ = F.interpolate(inp_img_small_, scale_factor=2)\n",
    "\n",
    "        mid_img = inp_img_mid + inp_img_small_\n",
    "\n",
    "        inp_enc_level1_mid = self.patch_embed_mid(mid_img)\n",
    "        out_enc_level1_mid = self.encoder_level1_mid1(inp_enc_level1_mid)\n",
    "\n",
    "        inp_enc_level2_mid = self.down1_2_mid(out_enc_level1_mid)\n",
    "        out_enc_level2_mid = self.encoder_level2_mid1(inp_enc_level2_mid)\n",
    "\n",
    "        inp_enc_level4_mid = self.down2_3_mid(out_enc_level2_mid)\n",
    "        latent_mid = self.latent_mid1(inp_enc_level4_mid)\n",
    "        latent_mid_INR_max = self.upmid2max1(latent_mid)\n",
    "        latent_mid_INR_max = self.upmid2max2(latent_mid_INR_max)\n",
    "\n",
    "        outputs.append(mid_img / 2)\n",
    "        INR2 = self.INR2(latent_mid_INR_max)\n",
    "        mid_img_ = INR2 + mid_img\n",
    "        outputs.append(mid_img_)\n",
    "\n",
    "        mid_img_ = F.interpolate(mid_img_, scale_factor=2)\n",
    "\n",
    "        max_img = inp_img_max + mid_img_\n",
    "\n",
    "        inp_enc_level1_max = self.patch_embed_max(max_img)\n",
    "        out_enc_level1_max = self.encoder_level1_max1(inp_enc_level1_max)\n",
    "\n",
    "        inp_enc_level2_max = self.down1_2_max(out_enc_level1_max)\n",
    "        out_enc_level2_max = self.encoder_level2_max1(inp_enc_level2_max)\n",
    "\n",
    "        inp_enc_level4_max = self.down2_3_max(out_enc_level2_max)\n",
    "        latent_max = self.latent_max1(inp_enc_level4_max)\n",
    "        BFF_max_1 = latent_max\n",
    "\n",
    "        inp_dec_level2_max = self.up3_2_max(latent_max)\n",
    "        inp_dec_level2_max = torch.cat([inp_dec_level2_max, out_enc_level2_max], 1)\n",
    "        inp_dec_level2_max = self.reduce_chan_level2_max1(inp_dec_level2_max)\n",
    "        out_dec_level2_max = self.decoder_level2_max1(inp_dec_level2_max)\n",
    "\n",
    "        inp_dec_level1_max = self.up2_1_max(out_dec_level2_max)\n",
    "        inp_dec_level1_max = torch.cat([inp_dec_level1_max, out_enc_level1_max], 1)\n",
    "        inp_dec_level1_max = self.reduce_chan_level1_max1(inp_dec_level1_max)\n",
    "        out_dec_level1_max = self.decoder_level1_max1(inp_dec_level1_max)\n",
    "\n",
    "        out_dec_level1_max = self.output_max_context1(out_dec_level1_max)\n",
    "        out_enc_level1_max = self.encoder_level1_max2(out_dec_level1_max)\n",
    "\n",
    "        inp_enc_level2_max = self.down1_2_max2(out_enc_level1_max)\n",
    "        out_enc_level2_max = self.encoder_level2_max2(inp_enc_level2_max)\n",
    "\n",
    "        inp_enc_level4_max = self.down2_3_max2(out_enc_level2_max)\n",
    "        latent_max = self.latent_max2(inp_enc_level4_max)\n",
    "        BFF_max_2 = latent_max\n",
    "\n",
    "        inp_dec_level2_max = self.up3_2_max2(latent_max)\n",
    "        inp_dec_level2_max = torch.cat([inp_dec_level2_max, out_enc_level2_max], 1)\n",
    "        inp_dec_level2_max = self.reduce_chan_level2_max2(inp_dec_level2_max)\n",
    "        out_dec_level2_max = self.decoder_level2_max2(inp_dec_level2_max)\n",
    "\n",
    "        inp_dec_level1_max = self.up2_1_max2(out_dec_level2_max)\n",
    "        inp_dec_level1_max = torch.cat([inp_dec_level1_max, out_enc_level1_max], 1)\n",
    "        inp_dec_level1_max = self.reduce_chan_level1_max2(inp_dec_level1_max)\n",
    "        out_dec_level1_max = self.decoder_level1_max2(inp_dec_level1_max)\n",
    "\n",
    "        out_dec_level1_max = self.output_max_context2(out_dec_level1_max)\n",
    "        out_enc_level1_max = self.encoder_level1_max3(out_dec_level1_max)\n",
    "\n",
    "        inp_enc_level2_max = self.down1_2_max3(out_enc_level1_max)\n",
    "        out_enc_level2_max = self.encoder_level2_max3(inp_enc_level2_max)\n",
    "\n",
    "        inp_enc_level4_max = self.down2_3_max3(out_enc_level2_max)\n",
    "        latent_max = self.latent_max3(inp_enc_level4_max)\n",
    "        BFF_max_3 = latent_max\n",
    "\n",
    "        BFF1 = self.BF1(BFF_max_1, BFF_max_2)\n",
    "        BFF2 = self.BF2(BFF_max_2, BFF_max_3)\n",
    "\n",
    "        BFF1 = F.interpolate(BFF1, scale_factor=0.5)\n",
    "        BFF2 = F.interpolate(BFF2, scale_factor=0.5)\n",
    "\n",
    "        inp_dec_level2_max = self.up3_2_max3(latent_max)\n",
    "\n",
    "        BFF3_1 = latent_mid\n",
    "        latent_mid = latent_mid + BFF1\n",
    "\n",
    "        inp_dec_level2_mid = self.up3_2_mid(latent_mid)\n",
    "        inp_dec_level2_mid = torch.cat([inp_dec_level2_mid, out_enc_level2_mid], 1)\n",
    "        inp_dec_level2_mid = self.reduce_chan_level2_mid1(inp_dec_level2_mid)\n",
    "        out_dec_level2_mid = self.decoder_level2_mid1(inp_dec_level2_mid)\n",
    "\n",
    "        inp_dec_level1_mid = self.up2_1_mid(out_dec_level2_mid)\n",
    "        inp_dec_level1_mid = torch.cat([inp_dec_level1_mid, out_enc_level1_mid], 1)\n",
    "        inp_dec_level1_mid = self.reduce_chan_level1_mid1(inp_dec_level1_mid)\n",
    "        out_dec_level1_mid = self.decoder_level1_mid1(inp_dec_level1_mid)\n",
    "\n",
    "        out_dec_level1_mid = self.output_mid_context(out_dec_level1_mid)\n",
    "        out_enc_level1_mid = self.encoder_level1_mid2(out_dec_level1_mid)\n",
    "\n",
    "        inp_enc_level2_mid = self.down1_2_mid2(out_enc_level1_mid)\n",
    "        out_enc_level2_mid = self.encoder_level2_mid2(inp_enc_level2_mid)\n",
    "\n",
    "        inp_enc_level4_mid = self.down2_3_mid2(out_enc_level2_mid)\n",
    "        latent_mid = self.latent_mid2(inp_enc_level4_mid)\n",
    "        BFF3_2 = latent_mid\n",
    "        BFF3 = self.BF3(BFF3_1, BFF3_2)\n",
    "        BFF3 = F.interpolate(BFF3, scale_factor=0.5)\n",
    "\n",
    "        latent_mid = latent_mid + BFF2\n",
    "\n",
    "        inp_dec_level2_mid = self.up3_2_mid2(latent_mid)\n",
    "\n",
    "        latent_small = latent_small + BFF3\n",
    "\n",
    "        inp_dec_level2_small = self.up3_2_small(latent_small)\n",
    "        inp_dec_level2_small = torch.cat([inp_dec_level2_small, out_enc_level2_small], 1)\n",
    "        inp_dec_level2_small = self.reduce_chan_level2_small(inp_dec_level2_small)\n",
    "        out_dec_level2_small = self.decoder_level2_small(inp_dec_level2_small)\n",
    "\n",
    "        inp_dec_level1_small = self.up2_1_small(out_dec_level2_small)\n",
    "        inp_dec_level1_small = torch.cat([inp_dec_level1_small, out_enc_level1_small], 1)\n",
    "        inp_dec_level1_small = self.reduce_chan_level1_small(inp_dec_level1_small)\n",
    "        out_dec_level1_small = self.decoder_level1_small(inp_dec_level1_small)\n",
    "\n",
    "        small_2_mid = out_dec_level1_small\n",
    "\n",
    "        out_dec_level1_small = self.output_small(out_dec_level1_small) + inp_img_small\n",
    "\n",
    "        outputs.append(out_dec_level1_small)\n",
    "        small = F.interpolate(out_dec_level1_small, scale_factor=2)\n",
    "\n",
    "        inp_dec_level2_mid = torch.cat([inp_dec_level2_mid, out_enc_level2_mid], 1)\n",
    "        inp_dec_level2_mid = self.reduce_chan_level2_mid2(inp_dec_level2_mid)\n",
    "        out_dec_level2_mid = self.decoder_level2_mid2(inp_dec_level2_mid)\n",
    "\n",
    "        inp_dec_level1_mid = self.up2_1_mid2(out_dec_level2_mid)\n",
    "        inp_dec_level1_mid = torch.cat([inp_dec_level1_mid, out_enc_level1_mid], 1)\n",
    "        inp_dec_level1_mid = self.reduce_chan_level1_mid2(inp_dec_level1_mid)\n",
    "        out_dec_level1_mid = self.decoder_level1_mid2(inp_dec_level1_mid)\n",
    "\n",
    "        small_2_mid = F.interpolate(small_2_mid, scale_factor=2)\n",
    "        out_dec_level1_mid = out_dec_level1_mid + small_2_mid\n",
    "\n",
    "        mid_2_max = out_dec_level1_mid\n",
    "\n",
    "        out_dec_level1_mid = self.output_mid(out_dec_level1_mid) + inp_img_mid\n",
    "\n",
    "        outputs.append(out_dec_level1_mid)\n",
    "        mid = F.interpolate(out_dec_level1_mid, scale_factor=2)\n",
    "\n",
    "        inp_dec_level2_max = torch.cat([inp_dec_level2_max, out_enc_level2_max], 1)\n",
    "        inp_dec_level2_max = self.reduce_chan_level2_max3(inp_dec_level2_max)\n",
    "        out_dec_level2_max = self.decoder_level2_max3(inp_dec_level2_max)\n",
    "\n",
    "        inp_dec_level1_max = self.up2_1_max3(out_dec_level2_max)\n",
    "        inp_dec_level1_max = torch.cat([inp_dec_level1_max, out_enc_level1_max], 1)\n",
    "        inp_dec_level1_max = self.reduce_chan_level1_max2(inp_dec_level1_max)\n",
    "        mid_2_max = F.interpolate(mid_2_max, scale_factor=2)\n",
    "        out_dec_level1_max = self.decoder_level1_max3(inp_dec_level1_max) + mid_2_max\n",
    "\n",
    "        out_dec_level1_max = self.output_max(out_dec_level1_max) + inp_img_max\n",
    "\n",
    "        outputs.append(out_dec_level1_max)\n",
    "\n",
    "        return outputs[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eebcee0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T08:42:08.299229Z",
     "iopub.status.busy": "2025-12-31T08:42:08.298960Z",
     "iopub.status.idle": "2025-12-31T08:42:08.307334Z",
     "shell.execute_reply": "2025-12-31T08:42:08.306808Z"
    },
    "papermill": {
     "duration": 0.026091,
     "end_time": "2025-12-31T08:42:08.308369",
     "exception": false,
     "start_time": "2025-12-31T08:42:08.282278",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "class GradualWarmupScheduler(_LRScheduler):\n",
    "    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n",
    "        self.multiplier = multiplier\n",
    "        if self.multiplier < 1.:\n",
    "            raise ValueError('multiplier should be greater thant or equal to 1.')\n",
    "        self.total_epoch = total_epoch\n",
    "        self.after_scheduler = after_scheduler\n",
    "        self.finished = False\n",
    "        super(GradualWarmupScheduler, self).__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch > self.total_epoch:\n",
    "            if self.after_scheduler:\n",
    "                if not self.finished:\n",
    "                    self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "                    self.finished = True\n",
    "                return self.after_scheduler.get_lr()\n",
    "            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "\n",
    "        if self.multiplier == 1.0:\n",
    "            return [base_lr * (float(self.last_epoch) / self.total_epoch) for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]\n",
    "\n",
    "    def step_ReduceLROnPlateau(self, metrics, epoch=None):\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "        self.last_epoch = epoch if epoch != 0 else 1  # ReduceLROnPlateau is called at the end of epoch, whereas others are called at beginning\n",
    "        if self.last_epoch <= self.total_epoch:\n",
    "            warmup_lr = [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]\n",
    "            for param_group, lr in zip(self.optimizer.param_groups, warmup_lr):\n",
    "                param_group['lr'] = lr\n",
    "        else:\n",
    "            if epoch is None:\n",
    "                self.after_scheduler.step(metrics, None)\n",
    "            else:\n",
    "                self.after_scheduler.step(metrics, epoch - self.total_epoch)\n",
    "\n",
    "    def step(self, epoch=None, metrics=None):\n",
    "        if type(self.after_scheduler) != ReduceLROnPlateau:\n",
    "            if self.finished and self.after_scheduler:\n",
    "                if epoch is None:\n",
    "                    self.after_scheduler.step(None)\n",
    "                else:\n",
    "                    self.after_scheduler.step(epoch - self.total_epoch)\n",
    "            else:\n",
    "                return super(GradualWarmupScheduler, self).step(epoch)\n",
    "        else:\n",
    "            self.step_ReduceLROnPlateau(metrics, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32f6f2ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T08:42:08.340805Z",
     "iopub.status.busy": "2025-12-31T08:42:08.340461Z",
     "iopub.status.idle": "2025-12-31T08:42:08.344814Z",
     "shell.execute_reply": "2025-12-31T08:42:08.344163Z"
    },
    "papermill": {
     "duration": 0.02177,
     "end_time": "2025-12-31T08:42:08.345901",
     "exception": false,
     "start_time": "2025-12-31T08:42:08.324131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dir = '/kaggle/input/rain13kdataset/train/train/Rain13K/'\n",
    "#val_dir = '/kaggle/input/rain13kdataset/test/test/Rain100H/'\n",
    "model_save_dir = '/kaggle/working/checkpoints/'\n",
    "mode = 'Deraining'\n",
    "session = 'Multiscale'\n",
    "patch_size = 128\n",
    "num_epochs = 20 #\n",
    "batch_size = 4\n",
    "val_epochs = 1\n",
    "start_lr = 1e-4\n",
    "end_lr = 1e-6\n",
    "warmup_epochs = 3\n",
    "\n",
    "resume = True #\n",
    "resume_ckpt = '/kaggle/input/checkpoints/Deraining/models/Multiscale/model_best.pth'\n",
    "\n",
    "model_dir = os.path.join(model_save_dir, mode, 'models', session)\n",
    "mkdir(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cc71784",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T08:42:08.378558Z",
     "iopub.status.busy": "2025-12-31T08:42:08.378032Z",
     "iopub.status.idle": "2025-12-31T08:42:13.486814Z",
     "shell.execute_reply": "2025-12-31T08:42:13.485912Z"
    },
    "papermill": {
     "duration": 5.126242,
     "end_time": "2025-12-31T08:42:13.487990",
     "exception": false,
     "start_time": "2025-12-31T08:42:08.361748",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total:  22893146\n",
      "Trainable:  22893146\n",
      "------------------------------------------------------------------------------\n",
      "==> Resuming Training with learning rate: 2.1936143029866096e-05\n",
      "------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model_restoration = MultiscaleNet()\n",
    "get_parameter_number(model_restoration)\n",
    "model_restoration.cuda()\n",
    "\n",
    "device_ids = [i for i in range(torch.cuda.device_count())]\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"\\n\\nLet's use\", torch.cuda.device_count(), \"GPUs!\\n\\n\")\n",
    "\n",
    "optimizer = optim.Adam(model_restoration.parameters(), lr=start_lr, betas=(0.9, 0.999), eps=1e-8)\n",
    "\n",
    "scheduler_cosine = optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs - warmup_epochs, eta_min=end_lr)\n",
    "scheduler = GradualWarmupScheduler(optimizer, multiplier=1, total_epoch=warmup_epochs, after_scheduler=scheduler_cosine)\n",
    "\n",
    "start_epoch = 1\n",
    "\n",
    "if resume:\n",
    "    load_checkpoint(model_restoration, resume_ckpt)\n",
    "    start_epoch = load_start_epoch(resume_ckpt) + 1\n",
    "    load_optim(optimizer, resume_ckpt)\n",
    "    for i in range(1, start_epoch):\n",
    "        scheduler.step()\n",
    "    new_lr = scheduler.get_lr()[0]\n",
    "    print('------------------------------------------------------------------------------')\n",
    "    print(\"==> Resuming Training with learning rate:\", new_lr)\n",
    "    print('------------------------------------------------------------------------------')\n",
    "\n",
    "if len(device_ids) > 1:\n",
    "    model_restoration = nn.DataParallel(model_restoration, device_ids=device_ids)\n",
    "\n",
    "criterion_L1 = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c03a5db4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T08:42:13.521961Z",
     "iopub.status.busy": "2025-12-31T08:42:13.521482Z",
     "iopub.status.idle": "2025-12-31T08:42:13.866543Z",
     "shell.execute_reply": "2025-12-31T08:42:13.865614Z"
    },
    "papermill": {
     "duration": 0.363211,
     "end_time": "2025-12-31T08:42:13.867907",
     "exception": false,
     "start_time": "2025-12-31T08:42:13.504696",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 13711\n",
      "Train: 10968 images\n",
      "Val:   2743 images\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "inp_files = sorted(os.listdir(os.path.join(train_dir, 'input')))\n",
    "tar_files = sorted(os.listdir(os.path.join(train_dir, 'target')))\n",
    "\n",
    "inp_paths = [os.path.join(train_dir, 'input', x) for x in inp_files if x.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "tar_paths = [os.path.join(train_dir, 'target', x) for x in tar_files if x.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "\n",
    "train_inp, val_inp, train_tar, val_tar = train_test_split(\n",
    "    inp_paths, tar_paths, \n",
    "    test_size=0.2,      \n",
    "    random_state=42     \n",
    ")\n",
    "\n",
    "print(f\"Total images: {len(inp_paths)}\")\n",
    "print(f\"Train: {len(train_inp)} images\")\n",
    "print(f\"Val:   {len(val_inp)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca1b46bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T08:42:13.904690Z",
     "iopub.status.busy": "2025-12-31T08:42:13.904110Z",
     "iopub.status.idle": "2025-12-31T08:42:13.911282Z",
     "shell.execute_reply": "2025-12-31T08:42:13.910574Z"
    },
    "papermill": {
     "duration": 0.026966,
     "end_time": "2025-12-31T08:42:13.912334",
     "exception": false,
     "start_time": "2025-12-31T08:42:13.885368",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Start Epoch 16 End Epoch 20\n",
      "===> Loading datasets\n"
     ]
    }
   ],
   "source": [
    "train_dataset = DataLoaderTrainCustom(train_inp, train_tar, {'patch_size': patch_size})\n",
    "val_dataset = DataLoaderValCustom(val_inp, val_tar, {'patch_size': patch_size})\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=0, \n",
    "    drop_last=False, \n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset, \n",
    "    batch_size=1, \n",
    "    shuffle=False, \n",
    "    num_workers=0, \n",
    "    drop_last=False, \n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print('===> Start Epoch {} End Epoch {}'.format(start_epoch, num_epochs))\n",
    "print('===> Loading datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667f091a",
   "metadata": {
    "papermill": {
     "duration": 0.01595,
     "end_time": "2025-12-31T08:42:13.944780",
     "exception": false,
     "start_time": "2025-12-31T08:42:13.928830",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85076ff2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T08:42:13.977787Z",
     "iopub.status.busy": "2025-12-31T08:42:13.977574Z",
     "iopub.status.idle": "2025-12-31T12:16:51.312153Z",
     "shell.execute_reply": "2025-12-31T12:16:51.311249Z"
    },
    "papermill": {
     "duration": 12877.352929,
     "end_time": "2025-12-31T12:16:51.313641",
     "exception": false,
     "start_time": "2025-12-31T08:42:13.960712",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2742 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "100%|██████████| 2742/2742 [38:26<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 16 PSNR: 29.3970 --- best_epoch 16 Best_PSNR 29.3970]\n",
      "------------------------------------------------------------------\n",
      "Epoch: 16\tTime: 2646.6941\tLoss: 70.9319\tLearningRate 0.000015\n",
      "------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2742/2742 [37:14<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 17 PSNR: 29.5573 --- best_epoch 17 Best_PSNR 29.5573]\n",
      "------------------------------------------------------------------\n",
      "Epoch: 17\tTime: 2553.7096\tLoss: 69.3319\tLearningRate 0.000009\n",
      "------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2742/2742 [37:14<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 18 PSNR: 29.6162 --- best_epoch 18 Best_PSNR 29.6162]\n",
      "------------------------------------------------------------------\n",
      "Epoch: 18\tTime: 2555.4831\tLoss: 68.6600\tLearningRate 0.000005\n",
      "------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2742/2742 [37:15<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 19 PSNR: 29.6816 --- best_epoch 19 Best_PSNR 29.6816]\n",
      "------------------------------------------------------------------\n",
      "Epoch: 19\tTime: 2558.0685\tLoss: 67.6977\tLearningRate 0.000003\n",
      "------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2742/2742 [37:17<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 20 PSNR: 29.7122 --- best_epoch 20 Best_PSNR 29.7122]\n",
      "------------------------------------------------------------------\n",
      "Epoch: 20\tTime: 2559.5564\tLoss: 67.0540\tLearningRate 0.000001\n",
      "------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_psnr = 0\n",
    "best_epoch = 0\n",
    "writer = SummaryWriter(model_dir)\n",
    "iter = 0\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    epoch_loss = 0\n",
    "    train_id = 1\n",
    "    \n",
    "    model_restoration.train()\n",
    "    for i, data in enumerate(tqdm(train_loader), 0):\n",
    "        for param in model_restoration.parameters():\n",
    "            param.grad = None\n",
    "        \n",
    "        target = data[0].cuda()\n",
    "        input_ = data[1].cuda()\n",
    "        restored = model_restoration(input_)\n",
    "        \n",
    "        loss_l1 = criterion_L1(restored[0], target)\n",
    "        \n",
    "        loss_l1.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss_l1.item()\n",
    "        iter += 1\n",
    "        writer.add_scalar('loss/l1_loss', loss_l1, iter)\n",
    "    \n",
    "    writer.add_scalar('loss/epoch_loss', epoch_loss, epoch)\n",
    "    \n",
    "    if epoch % val_epochs == 0:\n",
    "        model_restoration.eval()\n",
    "        psnr_val_rgb = []\n",
    "        for ii, data_val in enumerate((val_loader), 0):\n",
    "            target = data_val[0].cuda()\n",
    "            input_ = data_val[1].cuda()\n",
    "            with torch.no_grad():\n",
    "                restored = model_restoration(input_)\n",
    "            for res, tar in zip(restored[0], target):\n",
    "                psnr_val_rgb.append(torchPSNR(res, tar))\n",
    "        \n",
    "        psnr_val_rgb = torch.stack(psnr_val_rgb).mean().item()\n",
    "        writer.add_scalar('val/psnr', psnr_val_rgb, epoch)\n",
    "        if psnr_val_rgb > best_psnr:\n",
    "            best_psnr = psnr_val_rgb\n",
    "            best_epoch = epoch\n",
    "            torch.save({'epoch': epoch, 'state_dict': model_restoration.state_dict(), 'optimizer': optimizer.state_dict()}, os.path.join(model_dir, \"model_best.pth\"))\n",
    "        \n",
    "        print(\"[epoch %d PSNR: %.4f --- best_epoch %d Best_PSNR %.4f]\" % (epoch, psnr_val_rgb, best_epoch, best_psnr))\n",
    "        torch.save({'epoch': epoch, 'state_dict': model_restoration.state_dict(), 'optimizer': optimizer.state_dict()}, os.path.join(model_dir, f\"model_epoch_{epoch}.pth\"))\n",
    "    \n",
    "    scheduler.step()\n",
    "    print(\"------------------------------------------------------------------\")\n",
    "    print(\"Epoch: {}\\tTime: {:.4f}\\tLoss: {:.4f}\\tLearningRate {:.6f}\".format(epoch, time.time() - epoch_start_time, epoch_loss, scheduler.get_lr()[0]))\n",
    "    print(\"------------------------------------------------------------------\")\n",
    "    torch.save({'epoch': epoch, 'state_dict': model_restoration.state_dict(), 'optimizer': optimizer.state_dict()}, os.path.join(model_dir, \"model_latest.pth\"))\n",
    "\n",
    "writer.close()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 3777739,
     "sourceId": 6534542,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8810955,
     "sourceId": 13930904,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8829332,
     "sourceId": 14351875,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 12982.847734,
   "end_time": "2025-12-31T12:16:55.114159",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-31T08:40:32.266425",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
